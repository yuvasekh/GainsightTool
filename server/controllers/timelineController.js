const axios = require('axios');
const fs = require('fs').promises;
const path = require('path');

/**
 * ðŸš€ ENTERPRISE TIMELINE MIGRATION SYSTEM
 * 
 * MIGRATION MODES AVAILABLE:
 * 1. ENTERPRISE QUEUE MODE: For 40M+ records across 1529 users
 *    - 50+ concurrent user processing
 *    - Memory-efficient streaming (1K chunks)
 *    - Professional error handling & recovery
 *    - Estimated: 2-4 weeks for full 40M migration
 * 
 * 2. PARALLEL PROCESSING MODE: For smaller datasets
 *    - 10 activities processed simultaneously 
 *    - Enhanced retry logic with exponential backoff
 *    - 3-4x performance improvement over sequential
 * 
 * 3. SEQUENTIAL MODE: Original functionality (fallback)
 *    - Email-by-email processing
 *    - All existing error handling preserved
 * 
 * CONFIGURATION:
 * - Set USE_QUEUE_PROCESSING=true for enterprise queue mode (40M+ records)
 * - Set PARALLEL_PROCESSING_CONFIG.ENABLED=true for parallel mode
 * - Set both=false for sequential mode (original functionality)
 * - All existing functionality, error handling, and logging preserved
 */

// ðŸš€ ENTERPRISE QUEUE SYSTEM INTEGRATION
let migrationQueueManager = null;
try {
  const MigrationQueueManager = require('../services/migrationQueueManager');
  migrationQueueManager = new MigrationQueueManager(module.exports);
  console.log('âœ… Enterprise Queue System loaded for 40M+ record support');
} catch (error) {
  console.log('â„¹ï¸ Enterprise Queue System not available (Redis/BullMQ not configured)');
  console.log('ðŸ”„ Falling back to parallel/sequential processing modes');
}

// âš¡ PARALLEL PROCESSING CONFIGURATION
const PARALLEL_PROCESSING_CONFIG = {
  ENABLED: true,                    // Set to false to use sequential processing
  CONCURRENT_ACTIVITIES: 10,        // Number of activities to process simultaneously
  ENABLE_RETRY_LOGIC: true,         // Enable retry logic with exponential backoff
  MAX_RETRIES: 3,                   // Maximum number of retries per failed activity
  CHUNK_DELAY_MS: 200              // Delay between processing chunks (in milliseconds)
};

let userCookie = ""
exports.fetchTimeLine = async (req, res) => {
  try {
    const { instanceUrl, instanceToken, targetInstanceUrl, targetInstanceToken } = req.body;

    if (!instanceUrl || !instanceToken) {
      return res.status(400).json({ message: "Missing instance information" });
    }

    let allContent = [];
    let page = 0;
    const size = 20;

    while (true) {
      const url = `${instanceUrl}/v1/ant/timeline/search/activity?page=${page}&size=${size}`;

      const payload = {
        quickSearch: {},
        contextFilter: {},
        filterContext: "GLOBAL_TIMELINE"
      };

      const config = {
        method: 'post',
        url,
        headers: {
          'Content-Type': 'application/json',
          'Cookie': instanceToken
        },
        data: JSON.stringify(payload),
        maxBodyLength: Infinity
      };

      const response = await axios(config);

      const content = response?.data?.data?.content || [];
      allContent = [...allContent, ...content];

      const totalPages = response?.data?.data?.page?.totalPages;
      const currentPage = response?.data?.data?.page?.number;

      if (content.length === 0 || currentPage + 1 >= totalPages) {
        // Claude edited - Map company IDs for navigation links if target system info is provided
        if (targetInstanceUrl && targetInstanceToken && allContent.length > 0) {
          console.log("ðŸ” Claude edited - Starting company ID mapping for fetchTimeLine");
          // Process each timeline item to map company IDs
          for (let item of allContent) {
            if (item.contexts && item.contexts.length > 0) {
              for (let context of item.contexts) {
                if (context.obj === 'Company' && context.lbl) {
                  console.log(`ðŸ” Claude edited - Original company: ${context.lbl}, Original ID: ${context.id}`);
                  // Map company name to target system company ID
                  const targetCompanyId = await getCompanyIdByName(context.lbl, targetInstanceUrl, targetInstanceToken);
                  if (targetCompanyId) {
                    console.log(`âœ… Claude edited - Mapped ${context.lbl} from ${context.id} to ${targetCompanyId}`);
                    context.id = targetCompanyId; // Replace with target company ID
                  } else {
                    console.log(`âŒ Claude edited - Could not map company: ${context.lbl}`);
                  }
                }
              }
            }
          }
        } else {
          console.log("âš ï¸ Claude edited - No target system info provided for company ID mapping");
        }

        // Last page reached
        const fullResponse = {
          ...response.data,
          data: {
            ...response.data.data,
            content: allContent,
            page: {
              ...response.data.data.page,
              number: 0, // Reset to 0 if you want
              totalPages: 1, // Mark single combined response
              size: allContent.length,
              totalElements: allContent.length
            }
          },
        };
        return res.json(fullResponse);
      }

      page++;
    }

  } catch (error) {
    console.error("Error fetching timeline:", error.message);
    res.status(500).json({
      message: "Error fetching timeline",
      error: error.message
    });
  }
};
exports.CompanyTimeLine = async (req, res) => {
  try {
    // console.log(req.body, "req.body");
    const { instanceUrl, instanceToken, companyId, page = 0, size = 20, targetInstanceUrl, targetInstanceToken } = req.body;

    if (!instanceUrl || !instanceToken || !companyId) {
      return res.status(400).json({ message: "Missing instance information" });
    }

    const url = `${instanceUrl}/v1/ant/timeline/search/gsactivities?page=${page}&size=${size}&companyId=${companyId}`;

    const payload = {
      quickSearch: {},
      contextFilter: {},
      filterContext: "GLOBAL_TIMELINE",
    };

    const config = {
      method: 'post',
      url,
      headers: {
        'Content-Type': 'application/json',
        'Cookie': instanceToken, // assuming token is a cookie string like "JSESSIONID=..."
      },
      data: JSON.stringify(payload),
      maxBodyLength: Infinity,
    };

    const response = await axios(config);

    // Claude edited - Map company IDs for navigation links if target system info is provided
    if (targetInstanceUrl && targetInstanceToken && response.data?.data?.content) {
      console.log("ðŸ” Claude edited - Starting company ID mapping for CompanyTimeLine");
      const content = response.data.data.content;
      
      // Process each timeline item to map company IDs
      for (let item of content) {
        if (item.contexts && item.contexts.length > 0) {
          for (let context of item.contexts) {
            if (context.obj === 'Company' && context.lbl) {
              console.log(`ðŸ” Claude edited - Original company: ${context.lbl}, Original ID: ${context.id}`);
              // Map company name to target system company ID
              const targetCompanyId = await getCompanyIdByName(context.lbl, targetInstanceUrl, targetInstanceToken);
              if (targetCompanyId) {
                console.log(`âœ… Claude edited - Mapped ${context.lbl} from ${context.id} to ${targetCompanyId}`);
                context.id = targetCompanyId; // Replace with target company ID
              } else {
                console.log(`âŒ Claude edited - Could not map company: ${context.lbl}`);
              }
            }
          }
        }
      }
    } else {
      console.log("âš ï¸ Claude edited - No target system info provided for company ID mapping in CompanyTimeLine");
    }

    res.json(response.data);
  } catch (error) {
    console.error("Error fetching timeline:", error.message);
    res.status(500).json({
      message: "Error fetching timeline",
      error: error.message,
    });
  }
};
const { chromium } = require('playwright'); // Claude added for playwright functionality

// Migration Tracking and Logging Utility
const MigrationTracker = {
  // Initialize tracking data
  initializeTracking() {
    return {
      migrationId: this.generateMigrationId(),
      startTime: new Date(),
      endTime: null,
      totalDuration: null,
      totalProcessed: 0,
      successfulMigrations: [],
      failedMigrations: [],
      statistics: {
        totalCount: 0,
        successCount: 0,
        failureCount: 0,
        batchesProcessed: 0,
        averageTimePerActivity: null
      },
      batchTiming: [],
      errors: [],
      summary: null,
      emailStats: {}, // New: Track stats per email
      emailResults: {} // New: Track detailed results per email
    };
  },

  // Generate unique migration ID
  generateMigrationId() {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const randomId = Math.random().toString(36).substring(2, 8);
    return `migration_${timestamp}_${randomId}`;
  },

  // Track successful migration with entry ID
  trackSuccess(trackingData, sourceActivityId, targetActivityId, activityDetails = {}) {
    const successRecord = {
      entryId: sourceActivityId, // Added entry ID
      sourceActivityId: sourceActivityId,
      targetActivityId: targetActivityId,
      migratedAt: new Date().toISOString(),
      activityType: activityDetails.activityType || null,
      companyName: activityDetails.companyName || null,
      authorEmail: activityDetails.authorEmail || null,
      subject: activityDetails.subject || null,
      saveduser: activityDetails.saveduser || null
    };

    trackingData.successfulMigrations.push(successRecord);
    trackingData.statistics.successCount++;

    // Track per email stats
    const email = activityDetails.authorEmail || 'unknown';
    if (!trackingData.emailStats[email]) {
      trackingData.emailStats[email] = { successCount: 0, failureCount: 0 };
      trackingData.emailResults[email] = { successes: [], failures: [] };
    }
    trackingData.emailStats[email].successCount++;
    trackingData.emailResults[email].successes.push(successRecord);

    console.log(`âœ… SUCCESS: Entry ${sourceActivityId} â†’ ${targetActivityId} (${email})`);
  },

  // Track failed migration with entry ID
  trackFailure(trackingData, sourceActivityId, reason, activityDetails = {}) {
    const failureRecord = {
      entryId: sourceActivityId, // Added entry ID
      sourceActivityId: sourceActivityId,
      reason: reason,
      failedAt: new Date().toISOString(),
      activityType: activityDetails.activityType || null,
      companyName: activityDetails.companyName || null,
      authorEmail: activityDetails.authorEmail || null,
      subject: activityDetails.subject || null,
      errorCode: this.categorizeError(reason)
    };

    trackingData.failedMigrations.push(failureRecord);
    trackingData.statistics.failureCount++;

    // Track per email stats
    const email = activityDetails.authorEmail || 'unknown';
    if (!trackingData.emailStats[email]) {
      trackingData.emailStats[email] = { successCount: 0, failureCount: 0 };
      trackingData.emailResults[email] = { successes: [], failures: [] };
    }
    trackingData.emailStats[email].failureCount++;
    trackingData.emailResults[email].failures.push(failureRecord);

    console.log(`âŒ FAILURE: Entry ${sourceActivityId} - ${reason} (${email})`);
  },

  // Categorize errors for better analysis
  categorizeError(reason) {
    if (reason.includes('API')) return 'API_ERROR';
    if (reason.includes('validation')) return 'VALIDATION_ERROR';
    if (reason.includes('timeout')) return 'TIMEOUT_ERROR';
    if (reason.includes('authentication')) return 'AUTH_ERROR';
    if (reason.includes('company')) return 'COMPANY_MAPPING_ERROR';
    if (reason.includes('user')) return 'USER_MAPPING_ERROR';
    if (reason.includes('activity type')) return 'ACTIVITY_TYPE_ERROR';
    if (reason.includes('attachment')) return 'ATTACHMENT_ERROR';
    return 'UNKNOWN_ERROR';
  },

  // Track batch timing
  trackBatchTiming(trackingData, batchIndex, batchSize, startTime, endTime, successCount, failureCount) {
    const duration = endTime - startTime;
    const batchRecord = {
      batchIndex: batchIndex + 1,
      batchSize: batchSize,
      startTime: startTime.toISOString(),
      endTime: endTime.toISOString(),
      duration: duration,
      durationFormatted: this.formatDuration(duration),
      successCount: successCount,
      failureCount: failureCount,
      averageTimePerActivity: batchSize > 0 ? Math.round(duration / batchSize) : 0
    };

    trackingData.batchTiming.push(batchRecord);
    trackingData.statistics.batchesProcessed++;
  },

  // Track unexpected errors
  trackError(trackingData, error, context = '') {
    const errorRecord = {
      timestamp: new Date().toISOString(),
      context: context,
      message: error.message,
      stack: error.stack,
      type: error.constructor.name
    };

    trackingData.errors.push(errorRecord);
    console.error(`ðŸš¨ ERROR [${context}]: ${error.message}`);
  },

  // Finalize tracking and generate summary
  finalizeTracking(trackingData) {
    trackingData.endTime = new Date();
    trackingData.totalDuration = trackingData.endTime - trackingData.startTime;
    trackingData.totalProcessed = trackingData.statistics.successCount + trackingData.statistics.failureCount;

    // Calculate average time per activity
    if (trackingData.totalProcessed > 0) {
      trackingData.statistics.averageTimePerActivity = Math.round(trackingData.totalDuration / trackingData.totalProcessed);
    }

    // Generate comprehensive summary
    trackingData.summary = this.generateSummary(trackingData);

    return trackingData;
  },

  // Generate detailed summary report with email breakdown
  generateSummary(trackingData) {
    const successRate = trackingData.totalProcessed > 0
      ? ((trackingData.statistics.successCount / trackingData.totalProcessed) * 100).toFixed(2)
      : 0;

    // Generate email breakdown
    const emailBreakdown = Object.entries(trackingData.emailStats).map(([email, stats]) => ({
      email,
      successCount: stats.successCount,
      failureCount: stats.failureCount,
      totalProcessed: stats.successCount + stats.failureCount,
      successRate: stats.successCount + stats.failureCount > 0
        ? ((stats.successCount / (stats.successCount + stats.failureCount)) * 100).toFixed(2) + '%'
        : '0%'
    })).sort((a, b) => b.totalProcessed - a.totalProcessed);

    const summary = {
      migrationOverview: {
        migrationId: trackingData.migrationId,
        startTime: trackingData.startTime.toISOString(),
        endTime: trackingData.endTime.toISOString(),
        totalDuration: this.formatDuration(trackingData.totalDuration),
        totalDurationMs: trackingData.totalDuration
      },
      statistics: {
        totalActivitiesProcessed: trackingData.totalProcessed,
        successfulMigrations: trackingData.statistics.successCount,
        failedMigrations: trackingData.statistics.failureCount,
        successRate: `${successRate}%`,
        batchesProcessed: trackingData.statistics.batchesProcessed,
        averageTimePerActivity: `${trackingData.statistics.averageTimePerActivity}ms`,
        unexpectedErrors: trackingData.errors.length,
        emailsProcessed: Object.keys(trackingData.emailStats).length
      },
      emailBreakdown: emailBreakdown,
      performance: {
        totalBatches: trackingData.batchTiming.length,
        fastestBatch: this.getFastestBatch(trackingData.batchTiming),
        slowestBatch: this.getSlowestBatch(trackingData.batchTiming),
        averageBatchTime: this.getAverageBatchTime(trackingData.batchTiming)
      },
      errorAnalysis: this.analyzeErrors(trackingData.failedMigrations),
      systemAdminMappings: this.analyzeSystemAdminMappings(trackingData),
      detailedFailures: this.analyzeDetailedFailures(trackingData.failedMigrations)
    };

    return summary;
  },

  // Helper function to format duration
  formatDuration(milliseconds) {
    const seconds = Math.floor(milliseconds / 1000);
    const minutes = Math.floor(seconds / 60);
    const hours = Math.floor(minutes / 60);

    if (hours > 0) {
      return `${hours}h ${minutes % 60}m ${seconds % 60}s`;
    } else if (minutes > 0) {
      return `${minutes}m ${seconds % 60}s`;
    } else {
      return `${seconds}s ${milliseconds % 1000}ms`;
    }
  },

  // Analyze batch performance
  getFastestBatch(batchTiming) {
    if (batchTiming.length === 0) return null;
    const fastest = batchTiming.reduce((prev, current) =>
      (prev.duration < current.duration) ? prev : current
    );
    return {
      batchIndex: fastest.batchIndex,
      duration: this.formatDuration(fastest.duration),
      activitiesProcessed: fastest.batchSize
    };
  },

  getSlowestBatch(batchTiming) {
    if (batchTiming.length === 0) return null;
    const slowest = batchTiming.reduce((prev, current) =>
      (prev.duration > current.duration) ? prev : current
    );
    return {
      batchIndex: slowest.batchIndex,
      duration: this.formatDuration(slowest.duration),
      activitiesProcessed: slowest.batchSize
    };
  },

  getAverageBatchTime(batchTiming) {
    if (batchTiming.length === 0) return '0ms';
    const totalTime = batchTiming.reduce((sum, batch) => sum + batch.duration, 0);
    const averageTime = totalTime / batchTiming.length;
    return this.formatDuration(averageTime);
  },

  // Analyze error patterns
  analyzeErrors(failedMigrations) {
    const errorCounts = {};
    failedMigrations.forEach(failure => {
      const errorCode = failure.errorCode;
      errorCounts[errorCode] = (errorCounts[errorCode] || 0) + 1;
    });

    const sortedErrors = Object.entries(errorCounts)
      .sort(([, a], [, b]) => b - a)
      .map(([errorType, count]) => ({ errorType, count }));

    return {
      totalErrors: failedMigrations.length,
      errorBreakdown: sortedErrors,
      mostCommonError: sortedErrors[0] || null
    };
  },

  // Analyze system admin mappings
  analyzeSystemAdminMappings(trackingData) {
    const systemAdminUsers = [...new Set(trackingData.failedMigrations
      .filter(f => f.errorCode.includes('Source user inactive/not found in target system'))
      .map(f => f.errorCode.match(/\(([^)]+)\)/)?.[1])
      .filter(email => email))];
    
    return {
      totalUsersAssignedToAdmin: systemAdminUsers.length,
      usersList: systemAdminUsers
    };
  },

  // Analyze detailed failures by reason and user
  analyzeDetailedFailures(failedMigrations) {
    const companyNotFound = failedMigrations.filter(f => f.errorCode.includes('Company not found')).length;
    const draftFailed = failedMigrations.filter(f => f.errorCode.includes('Draft creation failed')).length;
    const userNotFound = failedMigrations.filter(f => f.errorCode.includes('Source user inactive/not found')).length;
    
    const failuresByUser = {};
    failedMigrations.forEach(failure => {
      const user = failure.activityDetails?.authorEmail || 'Unknown';
      if (!failuresByUser[user]) failuresByUser[user] = { total: 0, reasons: {} };
      failuresByUser[user].total++;
      const reason = failure.errorCode.includes('Company not found') ? 'Company ID not found' :
                     failure.errorCode.includes('Draft creation failed') ? 'Draft creation failed' :
                     failure.errorCode.includes('Source user inactive') ? 'User not found in target' : 'Other';
      failuresByUser[user].reasons[reason] = (failuresByUser[user].reasons[reason] || 0) + 1;
    });

    return {
      failureReasons: {
        companyIdNotFound: companyNotFound,
        draftCreationFailed: draftFailed - userNotFound,
        userNotFoundInTarget: userNotFound
      },
      failuresByUser: Object.entries(failuresByUser)
        .sort(([,a], [,b]) => b.total - a.total)
        .slice(0, 10)
        .map(([user, data]) => ({ user, ...data }))
    };
  },

  // Get next run number for organizing migration files
  async getNextRunNumber() {
    const baseDir = './migration-logs/runs';
    try {
      await fs.mkdir(baseDir, { recursive: true });
      const entries = await fs.readdir(baseDir, { withFileTypes: true });
      const runFolders = entries
        .filter(entry => entry.isDirectory() && entry.name.startsWith('run'))
        .map(entry => parseInt(entry.name.replace('run', '')))
        .filter(num => !isNaN(num));
      
      return runFolders.length > 0 ? Math.max(...runFolders) + 1 : 1;
    } catch (error) {
      console.log('ðŸ“ Creating first run folder');
      return 1;
    }
  },

  // Save tracking data to file
  async saveTrackingData(trackingData, outputDir = null) {
    try {
      // If no outputDir provided, create new run folder
      if (!outputDir) {
        const runNumber = await this.getNextRunNumber();
        outputDir = `./migration-logs/runs/run${runNumber}`;
        console.log(`ðŸ“ Using run folder: ${outputDir}`);
      }
      
      // Ensure output directory exists
      await fs.mkdir(outputDir, { recursive: true });

      const fileName = `migration_log_${trackingData.migrationId}.json`;
      const filePath = path.join(outputDir, fileName);

      // Save complete tracking data
      await fs.writeFile(filePath, JSON.stringify(trackingData, null, 2));
      console.log(`ðŸ“Š Migration tracking data saved to: ${filePath}`);

      // Save summary report separately
      const summaryFileName = `migration_summary_${trackingData.migrationId}.json`;
      const summaryFilePath = path.join(outputDir, summaryFileName);
      await fs.writeFile(summaryFilePath, JSON.stringify(trackingData.summary, null, 2));
      console.log(`ðŸ“‹ Migration summary saved to: ${summaryFilePath}`);

      // Save email-specific reports
      const emailReportFileName = `migration_email_breakdown_${trackingData.migrationId}.json`;
      const emailReportFilePath = path.join(outputDir, emailReportFileName);
      await fs.writeFile(emailReportFilePath, JSON.stringify(trackingData.emailResults, null, 2));
      console.log(`ðŸ“§ Email breakdown report saved to: ${emailReportFilePath}`);

      return {
        logFile: filePath,
        summaryFile: summaryFilePath,
        emailBreakdownFile: emailReportFilePath
      };

    } catch (error) {
      console.error('Error saving tracking data:', error.message);
      return null;
    }
  },

  // Save user-specific migration logs in individual email folders
  async saveUserSpecificLogs(trackingData, emailBreakdown, outputDir = './migration-logs') {
    try {
      const userLogs = {};
      
      // Create user-specific folders and logs
      for (const [email, stats] of Object.entries(emailBreakdown)) {
        if (!email || email === 'undefined') continue;
        
        // Create sanitized folder name from email
        const sanitizedEmail = email.replace(/[^a-zA-Z0-9@._-]/g, '_');
        const userDir = path.join(outputDir, 'user-logs', sanitizedEmail);
        
        // Ensure user directory exists
        await fs.mkdir(userDir, { recursive: true });
        
        // Filter activities for this specific user
        const userSuccessful = trackingData.successfulMigrations.filter(
          success => success.authorEmail === email
        );
        const userFailed = trackingData.failedMigrations.filter(
          failure => failure.authorEmail === email
        );
        
        // Create comprehensive user-specific migration report matching the required format
        const userReport = {
          message: stats.status === 'completed' ? "Migration completed successfully" : `Migration ${stats.status}`,
          migrationId: trackingData.migrationId,
          userEmail: email,
          totalProcessed: stats.totalActivities,
          successful: stats.successfulMigrations,
          failed: stats.failedMigrations,
          duration: trackingData.summary.migrationOverview.totalDuration,
          successRate: stats.successRate,
          summary: {
            migrationOverview: {
              migrationId: trackingData.migrationId,
              startTime: trackingData.summary.migrationOverview.startTime,
              endTime: trackingData.summary.migrationOverview.endTime,
              totalDuration: trackingData.summary.migrationOverview.totalDuration,
              totalDurationMs: trackingData.summary.migrationOverview.totalDurationMs,
              userEmail: email
            },
            statistics: {
              totalActivitiesProcessed: stats.totalActivities,
              successfulMigrations: stats.successfulMigrations,
              failedMigrations: stats.failedMigrations,
              successRate: stats.successRate,
              batchesProcessed: trackingData.summary.statistics.batchesProcessed,
              averageTimePerActivity: trackingData.summary.statistics.averageTimePerActivity,
              unexpectedErrors: userFailed.length,
              emailsProcessed: 1
            },
            emailBreakdown: [
              {
                email: email,
                successCount: stats.successfulMigrations,
                failureCount: stats.failedMigrations,
                totalProcessed: stats.totalActivities,
                successRate: stats.successRate
              }
            ],
            performance: {
              totalBatches: trackingData.summary.performance.totalBatches,
              fastestBatch: trackingData.summary.performance.fastestBatch,
              slowestBatch: trackingData.summary.performance.slowestBatch,
              averageBatchTime: trackingData.summary.performance.averageBatchTime
            },
            errorAnalysis: {
              totalErrors: userFailed.length,
              errorBreakdown: userFailed.reduce((acc, failure) => {
                const errorCode = failure.errorCode || 'UNKNOWN_ERROR';
                acc[errorCode] = (acc[errorCode] || 0) + 1;
                return acc;
              }, {}),
              mostCommonError: userFailed.length > 0 ? userFailed[0].errorCode : null
            }
          },
          files: {
            logFile: `user-logs/${sanitizedEmail}/migration_log_${sanitizedEmail}_${trackingData.migrationId}.json`,
            summaryFile: `user-logs/${sanitizedEmail}/migration_summary_${sanitizedEmail}_${trackingData.migrationId}.json`,
            emailBreakdownFile: `user-logs/${sanitizedEmail}/migration_email_breakdown_${sanitizedEmail}_${trackingData.migrationId}.json`
          },
          emailBreakdown: {
            [email]: {
              totalActivities: stats.totalActivities,
              successfulMigrations: stats.successfulMigrations,
              failedMigrations: stats.failedMigrations,
              successRate: stats.successRate,
              status: stats.status
            }
          },
          performance: {
            emailsProcessed: 1,
            pageSize: 2000
          },
          nextSteps: {
            totalActivitiesProcessed: stats.totalActivities,
            readyForNextPhase: stats.successfulMigrations > (stats.totalActivities * 0.8)
          },
          sampleSuccessfulEntries: userSuccessful.slice(0, 10).map(success => ({
            entryId: success.entryId,
            sourceActivityId: success.sourceActivityId,
            targetActivityId: success.targetActivityId,
            companyName: success.companyName,
            authorEmail: success.authorEmail
          })),
          sampleFailedEntries: userFailed.slice(0, 10).map(failure => ({
            entryId: failure.entryId,
            sourceActivityId: failure.sourceActivityId,
            reason: failure.reason,
            errorCode: failure.errorCode,
            companyName: failure.companyName,
            authorEmail: failure.authorEmail
          })),
          detailedResults: {
            successfulActivities: userSuccessful.map(success => ({
              entryId: success.entryId,
              sourceActivityId: success.sourceActivityId,
              targetActivityId: success.targetActivityId,
              companyName: success.companyName,
              authorEmail: success.authorEmail,
              migratedAt: success.migratedAt,
              activityType: success.activityType,
              subject: success.subject,
              saveduser: success.saveduser
            })),
            failedActivities: userFailed.map(failure => ({
              entryId: failure.entryId,
              sourceActivityId: failure.sourceActivityId,
              reason: failure.reason,
              errorCode: failure.errorCode,
              companyName: failure.companyName,
              authorEmail: failure.authorEmail,
              failedAt: failure.failedAt,
              activityType: failure.activityType,
              subject: failure.subject
            }))
          }
        };
        
        // Save user-specific log file
        const userLogFileName = `migration_log_${sanitizedEmail}_${trackingData.migrationId}.json`;
        const userLogFilePath = path.join(userDir, userLogFileName);
        await fs.writeFile(userLogFilePath, JSON.stringify(userReport, null, 2));
        
        console.log(`ðŸ“§ User-specific migration log saved for ${email}: ${userLogFilePath}`);
        
        userLogs[email] = {
          logFile: userLogFilePath,
          folder: userDir,
          stats: stats,
          report: userReport
        };
      }
      
      console.log(`ðŸ“ Created ${Object.keys(userLogs).length} user-specific migration log folders`);
      return userLogs;
      
    } catch (error) {
      console.error('Error saving user-specific logs:', error.message);
      return null;
    }
  },

  // Print console summary with email breakdown
  printConsoleSummary(trackingData) {
    const summary = trackingData.summary;

    console.log('\n' + '='.repeat(80));
    console.log('ðŸ“Š MIGRATION SUMMARY REPORT');
    console.log('='.repeat(80));
    console.log(`Migration ID: ${summary.migrationOverview.migrationId}`);
    console.log(`Duration: ${summary.migrationOverview.totalDuration}`);
    console.log(`Total Processed: ${summary.statistics.totalActivitiesProcessed}`);
    console.log(`âœ… Successful: ${summary.statistics.successfulMigrations}`);
    console.log(`âŒ Failed: ${summary.statistics.failedMigrations}`);
    console.log(`ðŸ“ˆ Success Rate: ${summary.statistics.successRate}`);
    console.log(`âš¡ Average Time per Activity: ${summary.statistics.averageTimePerActivity}`);
    console.log(`ðŸ“§ Emails Processed: ${summary.statistics.emailsProcessed}`);

    if (summary.emailBreakdown && summary.emailBreakdown.length > 0) {
      console.log('\nðŸ“§ Email Breakdown:');
      summary.emailBreakdown.forEach(emailStats => {
        console.log(`  â€¢ ${emailStats.email}: ${emailStats.totalProcessed} total (${emailStats.successCount} âœ…, ${emailStats.failureCount} âŒ) - ${emailStats.successRate} success`);
      });
    }

    if (summary.errorAnalysis.totalErrors > 0) {
      console.log('\nðŸ“‹ Error Breakdown:');
      summary.errorAnalysis.errorBreakdown.forEach(error => {
        console.log(`  â€¢ ${error.errorType}: ${error.count} occurrences`);
      });
    }

    console.log('='.repeat(80) + '\n');
  },

  // Load existing successful GSIDs from the delte folder
  async loadExistingSuccessfulGSIDs(outputDir = './migration-logs') {
    try {
      const delteDir = path.join(outputDir, 'delte');
      const gsidFilePath = path.join(delteDir, 'successful_gsids.json');
      
      try {
        const fileContent = await fs.readFile(gsidFilePath, 'utf8');
        const data = JSON.parse(fileContent);
        return data;
      } catch (error) {
        // File doesn't exist or is invalid, return empty structure
        return {
          totalSuccessfulGSIDs: 0,
          lastUpdated: null,
          users: {}
        };
      }
    } catch (error) {
      console.error('Error loading existing successful GSIDs:', error.message);
      return {
        totalSuccessfulGSIDs: 0,
        lastUpdated: null,
        users: {}
      };
    }
  },

  // Update successful GSIDs file with new migration results
  async updateSuccessfulGSIDs(trackingData, outputDir = './migration-logs') {
    try {
      const delteDir = path.join(outputDir, 'delte');
      const gsidFilePath = path.join(delteDir, 'successful_gsids.json');
      
      // Ensure delte directory exists
      await fs.mkdir(delteDir, { recursive: true });
      
      // Load existing data
      const existingData = await this.loadExistingSuccessfulGSIDs(outputDir);
      
      // Process new successful migrations
      const newSuccessfulGSIDs = {};
      let totalNewGSIDs = 0;
      
      trackingData.successfulMigrations.forEach(success => {
        const email = success.authorEmail || 'unknown';
        const gsid = success.targetActivityId;
        const companyName = success.companyName || 'Unknown Company';
        
        if (!newSuccessfulGSIDs[email]) {
          newSuccessfulGSIDs[email] = {
            email: email,
            gsids: [],
            count: 0,
            lastMigrationId: trackingData.migrationId,
            lastUpdated: new Date().toISOString()
          };
        }
        
        // Check if this GSID already exists for this user
        const existingUserGSIDs = existingData.users[email]?.gsids || [];
        const gsidExists = existingUserGSIDs.some(item => item.gsid === gsid);
        
        if (!gsidExists) {
          newSuccessfulGSIDs[email].gsids.push({
            gsid: gsid,
            sourceActivityId: success.sourceActivityId,
            companyName: companyName,
            migratedAt: success.migratedAt,
            migrationId: trackingData.migrationId,
            activityType: success.activityType,
            subject: success.subject
          });
          newSuccessfulGSIDs[email].count++;
          totalNewGSIDs++;
        }
      });
      
      // Merge with existing data
      for (const [email, newData] of Object.entries(newSuccessfulGSIDs)) {
        if (existingData.users[email]) {
          // User exists, merge GSIDs
          existingData.users[email].gsids = [...existingData.users[email].gsids, ...newData.gsids];
          existingData.users[email].count = existingData.users[email].gsids.length;
          existingData.users[email].lastMigrationId = trackingData.migrationId;
          existingData.users[email].lastUpdated = new Date().toISOString();
        } else {
          // New user
          existingData.users[email] = newData;
        }
      }
      
      // Update totals
      existingData.totalSuccessfulGSIDs = Object.values(existingData.users).reduce((total, user) => total + user.count, 0);
      existingData.lastUpdated = new Date().toISOString();
      existingData.lastMigrationId = trackingData.migrationId;
      existingData.migrationHistory = existingData.migrationHistory || [];
      
      // Add this migration to history
      existingData.migrationHistory.push({
        migrationId: trackingData.migrationId,
        timestamp: new Date().toISOString(),
        newGSIDsAdded: totalNewGSIDs,
        usersAffected: Object.keys(newSuccessfulGSIDs).length
      });
      
      // Keep only last 50 migration history entries
      if (existingData.migrationHistory.length > 50) {
        existingData.migrationHistory = existingData.migrationHistory.slice(-50);
      }
      
      // Save updated data
      await fs.writeFile(gsidFilePath, JSON.stringify(existingData, null, 2));
      
      console.log(`ðŸ“Š Successfully updated GSIDs file: ${gsidFilePath}`);
      console.log(`ðŸ“ˆ Added ${totalNewGSIDs} new GSIDs across ${Object.keys(newSuccessfulGSIDs).length} users`);
      console.log(`ðŸ“‹ Total GSIDs in database: ${existingData.totalSuccessfulGSIDs}`);
      
      return {
        filePath: gsidFilePath,
        totalGSIDs: existingData.totalSuccessfulGSIDs,
        newGSIDsAdded: totalNewGSIDs,
        usersAffected: Object.keys(newSuccessfulGSIDs).length
      };
      
    } catch (error) {
      console.error('Error updating successful GSIDs:', error.message);
      return null;
    }
  }
};

// Your existing constants and mappings remain the same
const ACTIVITY_TYPE_MAPPING = [
  { "Old Verizon Activity Type Name": "3G At-Risk Migration Update", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "3G Migration Update", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Admin Agent Update", "Activity Type": "Update", "Sub-Activity Type": "Admin Agent Update" },
  { "Old Verizon Activity Type Name": "Admin Intake Form", "Activity Type": "Update", "Sub-Activity Type": "Admin Intake Form" },
  { "Old Verizon Activity Type Name": "Adoption Discussion", "Activity Type": "Update", "Sub-Activity Type": "Adoption Discussion" },
  { "Old Verizon Activity Type Name": "At-Risk Customer Update", "Activity Type": "Update", "Sub-Activity Type": "At-Risk Customer Update" },
  { "Old Verizon Activity Type Name": "Call", "Activity Type": "Call", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Cancellation Request", "Activity Type": "Cancellation Request", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "CRT/PRM Update", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "CST - Adoption", "Activity Type": "Update", "Sub-Activity Type": "CST - Adoption" },
  { "Old Verizon Activity Type Name": "CST - Revenue Generating", "Activity Type": "Update", "Sub-Activity Type": "CST - Revenue Generating" },
  { "Old Verizon Activity Type Name": "CST - RYG", "Activity Type": "Update", "Sub-Activity Type": "CST - RYG" },
  { "Old Verizon Activity Type Name": "Customer Note", "Activity Type": "Update", "Sub-Activity Type": "Customer Note" },
  { "Old Verizon Activity Type Name": "EBR", "Activity Type": "Meeting", "Sub-Activity Type": "EBR" },
  { "Old Verizon Activity Type Name": "Email", "Activity Type": "Email", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "EST Update", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Ex-Customer Update", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Gov Cancellation Request", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Internal", "Activity Type": "Update", "Sub-Activity Type": "Internal" },
  { "Old Verizon Activity Type Name": "Meeting", "Activity Type": "Meeting", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Milestone", "Activity Type": "Milestone", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "Predictive Churn", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "QBR", "Activity Type": "Meeting", "Sub-Activity Type": "QBR" },
  { "Old Verizon Activity Type Name": "Renewal Update", "Activity Type": "Update", "Sub-Activity Type": "Renewal Update" },
  { "Old Verizon Activity Type Name": "Save-a-thon Campaign Activity", "Activity Type": "Update", "Sub-Activity Type": "" },
  { "Old Verizon Activity Type Name": "SSD Close Out", "Activity Type": "Update", "Sub-Activity Type": "SSD Close Out" },
  { "Old Verizon Activity Type Name": "Update", "Activity Type": "Update", "Sub-Activity Type": "" }
];

const MILESTONE_TYPE_MAPPING = [
  { "Old Milestone Type": "Adopting", "New Milestone Type": "Adopting" },
  { "Old Milestone Type": "CSM Assigned", "New Milestone Type": "CSM Transition" },
  { "Old Milestone Type": "CSM Transition", "New Milestone Type": "CSM Transition" },
  { "Old Milestone Type": "CSM Updates", "New Milestone Type": "CSM Updates" },
  { "Old Milestone Type": "Executive Business Review", "New Milestone Type": "Executive Business Review" },
  { "Old Milestone Type": "Expansion Opportunity Identified", "New Milestone Type": "Expansion Opportunity Identified" },
  { "Old Milestone Type": "Expansion Opportunity Won", "New Milestone Type": "Expansion Opportunity Won" },
  { "Old Milestone Type": "Health status -> Red", "New Milestone Type": "Risk Identified" },
  { "Old Milestone Type": "Launched", "New Milestone Type": "Launched" },
  { "Old Milestone Type": "Lifecycle Event Completed", "New Milestone Type": "Lifecycle Event Completed" },
  { "Old Milestone Type": "Objective Completed", "New Milestone Type": "Objective Completed" },
  { "Old Milestone Type": "Objective Created", "New Milestone Type": "Objective Created" },
  { "Old Milestone Type": "Onboarding Complete", "New Milestone Type": "Onboarding Complete" },
  { "Old Milestone Type": "Onboarding Kick-Off", "New Milestone Type": "Kicked Off" },
  { "Old Milestone Type": "Onboarding Started", "New Milestone Type": "Onboarding Started" },
  { "Old Milestone Type": "Predictive Churn Model", "New Milestone Type": "Will Churn" },
  { "Old Milestone Type": "Reference", "New Milestone Type": "Provided Reference" },
  { "Old Milestone Type": "Risk Identified", "New Milestone Type": "Risk Identified" },
  { "Old Milestone Type": "Risk Resolved", "New Milestone Type": "Risk Resolved" },
  { "Old Milestone Type": "Training", "New Milestone Type": "Training" },
  { "Old Milestone Type": "Upcoming Renewal", "New Milestone Type": "Lifecycle Event Created" },
  { "Old Milestone Type": "Will Churn", "New Milestone Type": "Will Churn" }
];

// Global Cache Variables (keeping your existing structure)
let userDataCache = null;
let companyDataCache = null;
let sourceActivityTypesCache = null;
let targetActivityTypesCache = null;
let subActivityTypesCache = null;
let meetingSubTypesCache = null;
let sourceMilestoneTypesCache = null;
let targetMilestoneTypesCache = null;

// Claude added - Cookie cache for playwright integration
const cookieCache = new Map(); // email -> cookie string

// Claude added - Function to map user by name for Salesforce activities
async function getUserIdByName(userName, targetInstanceUrl, targetInstanceToken) {
  console.log(`ðŸ” Looking up user by name: ${userName}`);
  
  try {
    // Get all target users from JSON file
    const targetUsers = await getAllTargetUsers(targetInstanceUrl, targetInstanceToken);
    
    if (!Array.isArray(targetUsers) || targetUsers.length === 0) {
      console.warn(`âš ï¸ No target users found in targetusers.json`);
      return null;
    }
    
    // Find user by exact name match
    const matchedUser = targetUsers.find(user => 
      user.Name && user.Name.trim() === userName.trim()
    );
    
    if (matchedUser) {
      console.log(`âœ… Found user by name: ${userName} -> ${matchedUser.GSID}`);
      return {
        GSID: matchedUser.GSID,
        Name: matchedUser.Name,
        Email: matchedUser.Email
      };
    }
    
    // Try case-insensitive match as fallback
    const matchedUserIgnoreCase = targetUsers.find(user => 
      user.Name && user.Name.toLowerCase().trim() === userName.toLowerCase().trim()
    );
    
    if (matchedUserIgnoreCase) {
      console.log(`âœ… Found user by name (case-insensitive): ${userName} -> ${matchedUserIgnoreCase.GSID}`);
      return {
        GSID: matchedUserIgnoreCase.GSID,
        Name: matchedUserIgnoreCase.Name,
        Email: matchedUserIgnoreCase.Email
      };
    }
    
    console.warn(`âš ï¸ No user found with name: ${userName}`);
    return null;
    
  } catch (error) {
    console.error(`âŒ Error looking up user by name ${userName}:`, error.message);
    return null;
  }
}

// Claude added - Function to fetch Gong audio data
async function fetchGongAudio(eid, sourceInstanceUrl, sourceInstanceToken, sourceActivityId) {
  console.log(`ðŸŽµ Fetching Gong audio for eid: ${eid}, sourceActivityId: ${sourceActivityId}`);
  
  try {
    // Construct the Gong API URL with the SOURCE activity ID (not target)
    const gongApiUrl = `${sourceInstanceUrl}/v1/ant/gongio/activity/${sourceActivityId}/${eid}`;
    
    console.log(`ðŸ“¡ Calling Gong API: ${gongApiUrl}`);
    
    const config = {
      method: 'GET',
      url: gongApiUrl,
      headers: {
        'Content-Type': 'application/json',
        'Cookie': sourceInstanceToken
      }
    };
    
    const response = await axios(config);
    
    if (response.data && response.data.result && response.data.data) {
      const audioData = response.data.data;
      console.log(`âœ… Successfully fetched Gong audio data for eid: ${eid}`);
      
      return {
        audioUrl: audioData.media?.audioUrl || null,
        videoUrl: audioData.media?.videoUrl || null,
        callUrl: audioData.callUrl || null,
        callId: audioData.callId || null,
        activityId: audioData.activityId || sourceActivityId
      };
    } else {
      console.warn(`âš ï¸ No audio data found for eid: ${eid}`);
      return null;
    }
    
  } catch (error) {
    console.error(`âŒ Error fetching Gong audio for eid ${eid}:`, error.message);
    throw error;
  }
}


// Function to read emails from JSON file
async function readEmailsFromFile() {
  const EMAILS_JSON_FILE = path.join(__dirname, 'emails.json');

  try {
    const fileExists = await fs.access(EMAILS_JSON_FILE).then(() => true).catch(() => false);
    if (!fileExists) {
      console.log('âŒ emails.json file does not exist. Creating example file...');
      return []
    }
    const fileContent = await fs.readFile(EMAILS_JSON_FILE, 'utf8');
    const emailData = JSON.parse(fileContent);

    // Support both array format and object with emails property
    if (Array.isArray(emailData)) {
      return emailData;
    } else if (emailData.emails && Array.isArray(emailData.emails)) {
      return emailData.emails;
    } else {
      throw new Error('Invalid emails.json format. Expected array or object with emails property.');
    }

  } catch (error) {
    console.error('âŒ Error reading emails from file:', error.message);
    console.log('ðŸ“ Using fallback email: eoin.mcmahon@verizonconnect.com');
    return ['eoin.mcmahon@verizonconnect.com'];
  }
}

// All your existing functions remain unchanged (normalizeString, getAllTargetUsers, etc.)
function normalizeString(str) {
  if (!str) return '';
  return str.toString().toLowerCase().trim().replace(/\s+/g, ' ');
}

const USERS_JSON_FILE = path.join(__dirname, 'targetusers.json');

async function getAllTargetUsers(instanceUrl, sessionCookie) {
  // Return cached data if available
  if (userDataCache) return userDataCache;

  try {
    const usersFromFile = await readUsersFromFile();

    if (Array.isArray(usersFromFile) && usersFromFile.length > 0) {
      console.log(`Loaded ${usersFromFile.length} users from JSON file`);
      userDataCache = usersFromFile;
    } else {
      console.warn('No users found in JSON file');
    }

    return userDataCache || [];

  } catch (error) {
    console.error('Error fetching all users:', error.message);
    return [];
  }
}

async function readUsersFromFile() {
  try {
    const fileExists = await fs.access(USERS_JSON_FILE).then(() => true).catch(() => false);

    if (!fileExists) {
      console.log('Users JSON file does not exist');
      return null;
    }

    const fileContent = await fs.readFile(USERS_JSON_FILE, 'utf8');
    const users = JSON.parse(fileContent);

    return users;
  } catch (error) {
    console.error('Error reading users from file:', error.message);
    return null;
  }
}

const COMPANIES_JSON_FILE = path.join(__dirname, 'targetcompanies.json');

async function getAllCompanies(instanceUrl, sessionCookie) {
  if (companyDataCache) return companyDataCache;

  try {
    const companiesFromFile = await readCompaniesFromFile();
    if (companiesFromFile && companiesFromFile.length > 0) {
      console.log(`Loaded ${companiesFromFile.length} companies from JSON file`);
      companyDataCache = companiesFromFile;
      return companiesFromFile;
    }

    return companiesFromFile;

  } catch (error) {
    console.error('Error fetching all companies:', error.message);
    return [];
  }
}

async function readCompaniesFromFile() {
  try {
    const fileExists = await fs.access(COMPANIES_JSON_FILE).then(() => true).catch(() => false);

    if (!fileExists) {
      console.log('Companies JSON file does not exist');
      return null;
    }

    const fileContent = await fs.readFile(COMPANIES_JSON_FILE, 'utf8');
    const companies = JSON.parse(fileContent);

    return companies;
  } catch (error) {
    console.error('Error reading companies from file:', error.message);
    return null;
  }
}

async function getAllActivityTypes(instanceUrl, sessionToken, cacheKey, companyId) {
  const cache = cacheKey === 'source' ? sourceActivityTypesCache : targetActivityTypesCache;
  if (cache) return cache;

  try {
    const config = {
      method: 'get',
      url: `${instanceUrl}/v1/ant/forms?context=Company&contextId=${companyId}&showHidden=false`,
      headers: { 'Cookie': sessionToken },
      timeout: 30000
    };

    const response = await axios(config);
    const activityTypes = response?.data?.data?.activityTypes || [];

    if (cacheKey === 'source') {
      sourceActivityTypesCache = activityTypes;
    } else {
      targetActivityTypesCache = activityTypes;
    }

    console.log(`Loaded ${activityTypes.length} activity types for ${cacheKey} system`);
    return activityTypes;
  } catch (error) {
    console.error(`Error fetching activity types for ${cacheKey}:`, error.message);
    return [];
  }
}

async function getSubActivityTypes(targetInstanceUrl, targetInstanceToken) {
  if (subActivityTypesCache) return subActivityTypesCache;

  try {
    const config = {
      method: 'get',
      url: `${targetInstanceUrl}/v1/ant/picklist/items/category/?ct=&id=1I00FBWECOTNM4VRRNBF3A8R43VMLERUJJFA&ref=`,
      headers: { 'Cookie': targetInstanceToken },
      timeout: 30000
    };

    const response = await axios(config);
    const subActivityTypes = response?.data?.data || [];

    subActivityTypesCache = subActivityTypes;
    console.log(`Loaded ${subActivityTypes.length} sub-activity types`);
    return subActivityTypes;
  } catch (error) {
    console.error('Error fetching sub-activity types:', error.message);
    return [];
  }
}

async function getMeetingSubTypes(targetInstanceUrl, targetInstanceToken) {
  if (meetingSubTypesCache) return meetingSubTypesCache;

  try {
    const config = {
      method: 'get',
      url: `${targetInstanceUrl}/v1/ant/picklist/items/category/?ct=&id=1I006DLKSIHRNJRYD2GHLL86BRHBL4WOY5U4&ref=`,
      headers: { 'Cookie': targetInstanceToken },
      timeout: 30000
    };

    const response = await axios(config);
    const meetingSubTypes = response?.data?.data || [];

    meetingSubTypesCache = meetingSubTypes;
    console.log(`Loaded ${meetingSubTypes.length} meeting sub-types`);
    return meetingSubTypes;
  } catch (error) {
    console.error('Error fetching meeting sub-types:', error.message);
    return [];
  }
}

async function getAdvancedActivityMapping(oldActivityTypeName, targetInstanceUrl, targetInstanceToken, sourceInstanceUrl, sourceInstanceToken, companyId) {
  try {
    console.log(`ðŸ” Mapping activity type: "${oldActivityTypeName}"`);

    const mapping = ACTIVITY_TYPE_MAPPING.find(m =>
      normalizeString(m["Old Verizon Activity Type Name"]) === normalizeString(oldActivityTypeName)
    );

    if (!mapping) {
      console.warn(`âš ï¸ No mapping found for "${oldActivityTypeName}"`);
      return { activityTypeId: null, subActivityTypeId: null };
    }

    console.log(`ðŸ“‹ Found mapping: ${oldActivityTypeName} â†’ ${mapping["Activity Type"]} / ${mapping["Sub-Activity Type"] || 'None'}`);

    const [targetActivityTypes, subActivityTypes, meetingSubTypes] = await Promise.all([
      getAllActivityTypes(targetInstanceUrl, targetInstanceToken, 'target', companyId),
      getSubActivityTypes(targetInstanceUrl, targetInstanceToken),
      getMeetingSubTypes(targetInstanceUrl, targetInstanceToken)
    ]);

    const targetActivityType = targetActivityTypes.find(type =>
      normalizeString(type.name) === normalizeString(mapping["Activity Type"])
    );

    if (!targetActivityType) {
      console.error(`âŒ Activity type "${mapping["Activity Type"]}" not found in target system`);
      return { activityTypeId: null, subActivityTypeId: null };
    }

    console.log(`âœ… Found main activity type: ${targetActivityType.name} (ID: ${targetActivityType.id})`);

    let subActivityTypeId = null;

    if (mapping["Sub-Activity Type"] && mapping["Sub-Activity Type"].trim() !== "") {
      const subActivityName = mapping["Sub-Activity Type"];

      if (normalizeString(mapping["Activity Type"]) === "meeting") {
        console.log(`ðŸ” Looking for meeting sub-type: "${subActivityName}"`);

        const meetingSubType = meetingSubTypes.find(type =>
          normalizeString(type.label) === normalizeString(subActivityName)
        );

        if (meetingSubType) {
          subActivityTypeId = meetingSubType.id;
          console.log(`âœ… Found meeting sub-type: ${meetingSubType.label} (ID: ${subActivityTypeId})`);
        } else {
          console.warn(`âš ï¸ Meeting sub-type "${subActivityName}" not found`);
        }
      } else {
        console.log(`ðŸ” Looking for sub-activity type: "${subActivityName}"`);

        const subActivityType = subActivityTypes.find(type =>
          normalizeString(type.label) === normalizeString(subActivityName)
        );

        if (subActivityType) {
          subActivityTypeId = subActivityType.id;
          console.log(`âœ… Found sub-activity type: ${subActivityType.label} (ID: ${subActivityTypeId})`);
        } else {
          console.warn(`âš ï¸ Sub-activity type "${subActivityName}" not found`);
        }
      }
    }

    return {
      activityTypeId: targetActivityType.id,
      subActivityTypeId: subActivityTypeId,
      activityTypeName: targetActivityType.name,
      subActivityTypeName: mapping["Sub-Activity Type"] || null
    };

  } catch (error) {
    console.error('Error in advanced activity mapping:', error.message);
    return { activityTypeId: null, subActivityTypeId: null };
  }
}

async function getSourceMilestoneTypes(sourceInstanceUrl, sourceInstanceToken, companyId) {
  if (sourceMilestoneTypesCache) return sourceMilestoneTypesCache;

  try {
    const config = {
      method: 'get',
      url: `${sourceInstanceUrl}/v1/ant/picklist/items/by/Company?cid=${companyId}&ct=MILESTONE&id=&ref=SCRIBBLE`,
      headers: { 'Cookie': sourceInstanceToken },
      timeout: 30000
    };

    const response = await axios(config);
    const milestoneTypes = response?.data?.data || [];

    sourceMilestoneTypesCache = milestoneTypes;
    console.log(`Loaded ${milestoneTypes.length} milestone types from SOURCE system`);
    return milestoneTypes;
  } catch (error) {
    console.error('Error fetching source milestone types:', error.message);
    return [];
  }
}

async function getTargetMilestoneTypes(targetInstanceUrl, targetInstanceToken, companyId) {
  if (targetMilestoneTypesCache) return targetMilestoneTypesCache;

  try {
    const config = {
      method: 'get',
      url: `${targetInstanceUrl}/v1/ant/picklist/items/by/Company?cid=${companyId}&ct=MILESTONE&id=&ref=SCRIBBLE`,
      headers: { 'Cookie': targetInstanceToken },
      timeout: 30000
    };

    const response = await axios(config);
    const milestoneTypes = response?.data?.data || [];

    targetMilestoneTypesCache = milestoneTypes;
    console.log(`Loaded ${milestoneTypes.length} milestone types from TARGET system`);
    return milestoneTypes;
  } catch (error) {
    console.error('Error fetching target milestone types:', error.message);
    return [];
  }
}

async function getMilestoneTypeMapping(oldMilestoneTypeId, sourceInstanceUrl, sourceInstanceToken, targetInstanceUrl, targetInstanceToken, sourceCompanyId, targetCompanyId) {
  try {
    console.log(`ðŸ” Mapping milestone type ID: "${oldMilestoneTypeId}"`);

    if (!oldMilestoneTypeId || oldMilestoneTypeId.trim() === '') {
      console.warn('âš ï¸ No milestone type ID provided');
      return null;
    }

    const [sourceMilestoneTypes, targetMilestoneTypes] = await Promise.all([
      getSourceMilestoneTypes(sourceInstanceUrl, sourceInstanceToken, sourceCompanyId),
      getTargetMilestoneTypes(targetInstanceUrl, targetInstanceToken, targetCompanyId)
    ]);

    const sourceMilestoneType = sourceMilestoneTypes.find(type => type.id === oldMilestoneTypeId);
    if (!sourceMilestoneType) {
      console.error(`âŒ Old milestone type ID not found: "${oldMilestoneTypeId}"`);
      return null;
    }

    const oldMilestoneLabel = sourceMilestoneType.label;
    console.log(`âœ… Found old milestone type: "${oldMilestoneLabel}" (ID: ${oldMilestoneTypeId})`);

    const mapping = MILESTONE_TYPE_MAPPING.find(m =>
      normalizeString(m["Old Milestone Type"]) === normalizeString(oldMilestoneLabel)
    );

    if (!mapping) {
      console.error(`âŒ No business logic mapping found for: "${oldMilestoneLabel}"`);
      return null;
    }

    const newMilestoneLabel = mapping["New Milestone Type"];
    console.log(`âœ… Found business logic mapping: "${oldMilestoneLabel}" â†’ "${newMilestoneLabel}"`);

    const targetMilestoneType = targetMilestoneTypes.find(type =>
      normalizeString(type.label) === normalizeString(newMilestoneLabel)
    );

    if (!targetMilestoneType) {
      console.error(`âŒ New milestone type not found in target system: "${newMilestoneLabel}"`);
      return null;
    }

    const newMilestoneTypeId = targetMilestoneType.id;
    console.log(`âœ… Found new milestone type: "${targetMilestoneType.label}" (ID: ${newMilestoneTypeId})`);

    return {
      oldMilestoneTypeId: oldMilestoneTypeId,
      oldMilestoneLabel: oldMilestoneLabel,
      newMilestoneLabel: targetMilestoneType.label,
      newMilestoneTypeId: newMilestoneTypeId,
      mappingRule: `"${oldMilestoneLabel}" â†’ "${newMilestoneLabel}"`
    };

  } catch (error) {
    console.error('Error in milestone type mapping:', error.message);
    return null;
  }
}

// Claude added - Function to get user cookie via playwright
async function getUserCookieViaPlaywright(targetEmail) {
  console.log(`ðŸŽ­ Getting cookie for user: ${targetEmail} via Playwright`);
  if (typeof userCookie === "object") {
    return userCookie;
  }

  const browser = await chromium.launch({ headless: true });
  const context = await browser.newContext();
  const page = await context.newPage();

  try {
    // Login as admin
    console.log("ðŸŒ Navigating to login page...");
    await page.goto("https://vznconnect.gainsightcloud.com");

    await page.waitForSelector('input[name="username"]', { timeout: 30000 });
    await page.waitForSelector('input[name="password"]', { timeout: 30000 });
    await page.fill('input[name="username"]', "sugandha.joshi@wigmoreit.com");
    await page.fill('input[name="password"]', "ToGainSight@14");

    console.log("ðŸ” Submitting login...");
    await Promise.all([
      page.waitForNavigation({ waitUntil: "networkidle" }),
      page.keyboard.press("Enter"),
    ]);

    // Navigate to User Management
    console.log("âž¡ï¸ Navigating to User Management page...");
    await page.goto("https://vznconnect.gainsightcloud.com/v1/ui/usermanagement#/users", {
      waitUntil: "networkidle",
    });

    // Search for user
    console.log(`ðŸ” Searching for user with email: ${targetEmail}`);
    const searchInput = page.locator('input.px-search.ant-input[placeholder="Name or Email"]');
    await searchInput.waitFor({ timeout: 10000 });
    await searchInput.clear();
    await searchInput.fill(targetEmail);
    await searchInput.press("Enter");
    await page.waitForTimeout(2000);

    // Find and click three dots menu
    const threeDotButton = page.locator('svg[data-icon="more-vertical"]').first();
    await threeDotButton.waitFor({ timeout: 10000 });
    await threeDotButton.click();
    await page.waitForTimeout(1000);

    // Click "Login as User"
    const loginMenuItem = page.locator('li.ant-menu-item:has-text("Login as User")');
    await loginMenuItem.waitFor({ timeout: 10000 });

    const pagePromise = context.waitForEvent("page");
    await loginMenuItem.click();
    const newPage = await pagePromise;

    // Wait for login to complete
    await newPage.waitForLoadState("networkidle");

    // Get cookies from the new page
    const cookies = await newPage.context().cookies();
    console.log(`ðŸ“„ Retrieved ${cookies.length} cookies for user: ${targetEmail}`);

    // Format cookies as required (sid="value")
    const formattedCookies = cookies.map(cookie => `${cookie.name}="${cookie.value}"`).join('; ');

    // Save cookies to file in the format expected
    const timestamp = Date.now();
    const cookieFileName = `login_as_user_cookies_${timestamp}.json`;
    const cookieFilePath = path.join(__dirname, cookieFileName);

    // Convert to the format needed - array of cookie objects
    const cookieData = cookies.map(cookie => ({
      name: cookie.name,
      value: cookie.value,
      domain: cookie.domain,
      path: cookie.path,
      expires: cookie.expires,
      httpOnly: cookie.httpOnly,
      secure: cookie.secure,
      sameSite: cookie.sameSite
    }));

    await fs.writeFile(cookieFilePath, JSON.stringify(cookieData, null, 2));
    console.log(`ðŸ’¾ Saved cookies to: ${cookieFilePath}`);

    await newPage.close();
    await browser.close();
    userCookie = {
      cookieString: formattedCookies,
      cookieFile: cookieFilePath,
      cookies: cookieData
    };
    return {
      cookieString: formattedCookies,
      cookieFile: cookieFilePath,
      cookies: cookieData
    };

  } catch (error) {
    console.error(`âŒ Error getting cookie for ${targetEmail}:`, error.message);
    await browser.close();
    throw error;
  }
}

// Claude added - Function to get user cookie with cache and retry logic
async function getUserCookieWithCache(userEmail, targetInstanceUrl) {
  // ðŸš€ ENTERPRISE COOKIE MANAGER - ULTRA-FAST COOKIE RETRIEVAL
  if (enterpriseCookieManager && enterpriseCookieManager.isReady()) {
    try {
      const cachedCookie = await enterpriseCookieManager.getCachedCookie(userEmail, targetInstanceUrl);
      console.log(`âš¡ [ULTRA-FAST] Using pre-loaded cookie for: ${userEmail} (no Playwright delay!)`);
      return cachedCookie;
    } catch (cacheError) {
      console.warn(`âš ï¸ [COOKIE CACHE] Failed to get cached cookie for ${userEmail}:`, cacheError.message);
      console.log(`ðŸ”„ [COOKIE CACHE] Falling back to standard cookie retrieval`);
      // Fall through to standard cookie handling
    }
  }

  // Claude added - Special handling for Sugandha admin user - no Playwright needed
  if (userEmail && userEmail.toLowerCase() === 'sugandha.joshi@wigmoreit.com') {
    console.log(`ðŸ”‘ Using Sugandha admin cookie (no Playwright) for user: ${userEmail}`);
    try {
      const SUGANDHA_COOKIE_FILE = path.join(__dirname, 'sugandha cookie');
      const cookieContent = await fs.readFile(SUGANDHA_COOKIE_FILE, 'utf8');
      const sugandhaookie = cookieContent.trim().replace(/^["']|["']$/g, ''); // Remove quotes if present
      
      // Test the Sugandha cookie
      try {
        const testResponse = await axios({
          method: 'get',
          url: `${targetInstanceUrl}/v1/ant/user/profile`,
          headers: { 'Cookie': sugandhaookie },
          timeout: 10000
        });

        if (testResponse.status === 200) {
          console.log(`âœ… Sugandha admin cookie verified for: ${userEmail}`);
          // Cache the cookie for future use
          cookieCache.set(userEmail, sugandhaookie);
          return sugandhaookie;
        }
      } catch (testError) {
        console.warn(`âš ï¸ Sugandha cookie test failed:`, testError.message);
        throw new Error(`Sugandha admin cookie is invalid - migration cannot proceed for ${userEmail}`);
      }
    } catch (error) {
      console.error(`âŒ Error reading Sugandha cookie file:`, error.message);
      throw new Error(`Cannot read Sugandha cookie file - migration cannot proceed for ${userEmail}`);
    }
  }

  const maxRetries = 2;
  let retryCount = 0;

  while (retryCount <= maxRetries) {
    // Check cache first (but skip cache on retry)
    if (retryCount === 0 && cookieCache.has(userEmail)) {
      console.log(`âœ… Using cached cookie for: ${userEmail}`);
      const cachedCookie = cookieCache.get(userEmail);

      // Test cached cookie
      try {
        const testResponse = await axios({
          method: 'get',
          url: `${targetInstanceUrl}/v1/ant/user/profile`,
          headers: { 'Cookie': cachedCookie },
          timeout: 10000
        });

        if (testResponse.status === 200) {
          console.log(`âœ… Cached cookie verified for: ${userEmail}`);
          return cachedCookie;
        }
      } catch (testError) {
        console.warn(`âš ï¸ Cached cookie expired for ${userEmail}, getting fresh one`);
        cookieCache.delete(userEmail); // Claude added - clear expired cookie from cache
      }
    }

    try {
      // Get fresh cookie via playwright
      const cookieResult = await getUserCookieViaPlaywright(userEmail);
      const cookieString = cookieResult.cookieString;

      // Test the cookie by making a simple API call
      try {
        const testResponse = await axios({
          method: 'get',
          url: `${targetInstanceUrl}/v1/ant/user/profile`,
          headers: { 'Cookie': cookieString },
          timeout: 10000
        });

        if (testResponse.status === 200) {
          console.log(`âœ… Fresh cookie verified for: ${userEmail}`);
          cookieCache.set(userEmail, cookieString);
          return cookieString;
        }
      } catch (testError) {
        console.warn(`âš ï¸ Fresh cookie test failed for ${userEmail}, cookie might be invalid`);
      }

      // Cache the cookie even if test failed (might work for the actual API call)
      cookieCache.set(userEmail, cookieString);
      return cookieString;

    } catch (error) {
      retryCount++;
      console.error(`âŒ Attempt ${retryCount} failed to get cookie for ${userEmail}:`, error.message);

      if (retryCount > maxRetries) {
        console.error(`âŒ All ${maxRetries + 1} attempts failed to get cookie for ${userEmail}`);
        throw error;
      }

      // Wait before retry
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
  }
}

async function getUserIdByEmail(email, instanceUrl, sessionCookie) {
  try {
    const users = await getAllTargetUsers(instanceUrl, sessionCookie);

    let user = users.find(u => u.Email?.toLowerCase() === email.toLowerCase());

    // If not found, fallback to no-reply@gainsightapp.com
    if (!user) {
      console.warn(`User with email ${email} not found. Falling back to no-reply@gainsightapp.com`);
      user = users.find(u => u.Email?.toLowerCase() === 'no-reply@gainsightapp.com');
      // Mark that this user was mapped to system admin
      if (user) {
        user.mappedToSystemAdmin = true;
        user.originalEmail = email;
      }
    }

    return user;
  } catch (error) {
    console.error(`Error getting user ID by email (${email}):`, error.message);
    return "1P01E316G9DAPFOLE6SOOUG71XRMN5F3PLER";
  }
}

async function getCompanyIdByName(companyName, instanceUrl, sessionCookie) {
  try {
    const companies = await getAllCompanies(instanceUrl, sessionCookie);
    const company = companies.find(c => c.Name?.toLowerCase() === companyName.toLowerCase());
    return company ? company.GSID : null;
  } catch (error) {
    console.error(`Error getting company ID by name (${companyName}):`, error.message);
    return "";
  }
}

async function downloadAttachment(attachmentUrl, attachmentName) {
  try {
    const response = await axios({
      method: 'GET',
      url: attachmentUrl,
      responseType: 'arraybuffer'
    });

    return {
      buffer: response.data,
      name: attachmentName,
      contentType: response.headers['content-type'] || 'application/octet-stream'
    };
  } catch (error) {
    console.error(`Error downloading attachment ${attachmentName}:`, error.message);
    throw error;
  }
}

async function uploadAttachment(attachmentData, companyId, companyLabel, userId, userName, userEmail, targetInstanceUrl, targetInstanceToken) {
  try {
    const FormData = require('form-data');
    const form = new FormData();
    console.log(attachmentData, "attachmentData")

    form.append('file', attachmentData.buffer, {
      filename: attachmentData.name,
      contentType: attachmentData.contentType
    });

    const requestPayload = {
      entityId: null,
      contexts: [{
        id: companyId,
        obj: "Company",
        eobj: "Account",
        eid: "",
        esys: "SALESFORCE",
        lbl: companyLabel,
        dsp: true,
        base: true
      }],
      source: "C360",
      user: {
        id: userId,
        obj: "User",
        name: userName,
        email: userEmail,
        eid: "",
        eobj: "User",
        epp: null,
        esys: "SALESFORCE",
        sys: "GAINSIGHT",
        pp: ""
      },
      type: "DEFAULT"
    };

    form.append('requestString', JSON.stringify(requestPayload));

    const config = {
      method: 'post',
      url: `${targetInstanceUrl}/v1/ant/attachments`,
      headers: {
        'Cookie': targetInstanceToken,
        ...form.getHeaders()
      },
      data: form,
      maxBodyLength: Infinity,
      maxContentLength: Infinity
    };

    const response = await axios(config);
    return response.data.data;

  } catch (error) {
    console.error(`Error uploading attachment ${attachmentData.name}:`, error.message);
    throw error;
  }
}

async function processAttachments(attachments, companyId, companyLabel, userId, userName, userEmail, targetInstanceUrl, targetInstanceToken) {
  const uploadedAttachments = [];

  for (const attachment of attachments) {
    try {
      console.log(`Processing attachment: ${attachment.name}`);

      const attachmentData = await downloadAttachment(attachment.url, attachment.name);

      const uploadResult = await uploadAttachment(
        attachmentData,
        companyId,
        companyLabel,
        userId,
        userName,
        userEmail,
        targetInstanceUrl,
        targetInstanceToken
      );
      console.log(uploadResult, "uploadResult")

      const processedAttachment = {
        id: uploadResult.id || uploadResult.attachmentId,
        seqId: uploadResult.seqId || attachment.seqId,
        name: attachment.name,
        url: uploadResult.url || attachment.url,
        size: attachment.size,
        removed: false,
        published: true,
        type: attachment.type,
        createdDate: uploadResult.createdDate || new Date().toISOString(),
        connectionId: null,
        syncedToExtSys: false,
        eid: null,
        esys: null
      };

      uploadedAttachments.push(processedAttachment);
      console.log(`Successfully uploaded attachment: ${attachment.name}`);

      await new Promise(resolve => setTimeout(resolve, 500));

    } catch (error) {
      console.error(`Failed to process attachment ${attachment.name}:`, error.message);
    }
  }

  return uploadedAttachments;
}

async function createDraft(draftPayload, targetInstanceUrl, targetInstanceToken) {
  try {
    const response = await axios({
      method: 'post',
      url: `${targetInstanceUrl}/v1/ant/v2/activity/drafts`,
      headers: {
        'Content-Type': 'application/json',
        'Cookie': targetInstanceToken
      },
      data: JSON.stringify(draftPayload),
      maxBodyLength: Infinity,
      timeout: 60000 // 60 seconds timeout for large payloads
    });

    console.log(response?.data, "yuva899");
    return response?.data?.data?.id || null;
  } catch (err) {
    console.error("Failed to create draft:", err.message);
    return null;
  }
}

// Claude added - Load external attendees mapping data
let externalAttendeesData = null;

// Claude added - Function to load external attendees data
async function loadExternalAttendeesData() {
  try {
    if (!externalAttendeesData) {
      const data = await fs.readFile('/Users/ramprasadsomaraju/Desktop/GS Migration/GainsightTool-dev2/externalAttendees.json', 'utf8');
      externalAttendeesData = JSON.parse(data);
      console.log(`âœ… Loaded ${externalAttendeesData.length} external attendees records`);
    }
    return externalAttendeesData;
  } catch (error) {
    console.error('âŒ Failed to load external attendees data:', error.message);
    return [];
  }
}

// Claude added - Function to map external attendees from source to target
async function mapExternalAttendees(sourceExternalAttendees, targetCompanyId) {
  if (!sourceExternalAttendees || !Array.isArray(sourceExternalAttendees) || sourceExternalAttendees.length === 0) {
    return [];
  }

  const externalData = await loadExternalAttendeesData();
  const mappedAttendees = [];

  for (const sourceAttendee of sourceExternalAttendees) {
    const attendeeEmail = sourceAttendee.email;
    
    if (!attendeeEmail) {
      console.warn('âš ï¸ External attendee missing email, skipping');
      continue;
    }

    // Find matching attendee in external data by email
    const matchingAttendee = externalData.find(extAttendee => 
      extAttendee.email && extAttendee.email.toLowerCase() === attendeeEmail.toLowerCase()
    );

    if (matchingAttendee) {
      // Map the external attendee to target format
      const mappedAttendee = {
        companyId: targetCompanyId, // Use target company ID for consistency
        email: attendeeEmail,
        epp: null,
        esys: "",
        id: matchingAttendee.company_person_gsid, // Map to company_person_gsid
        name: matchingAttendee.name || sourceAttendee.name || '', // Use name from mapping or source
        personId: matchingAttendee.person_id, // Map to person_id
        pp: null,
        type: sourceAttendee.type || "Contact"
      };
      
      mappedAttendees.push(mappedAttendee);
      console.log(`âœ… Mapped external attendee: ${attendeeEmail} -> ${matchingAttendee.company_person_gsid}`);
    } else {
      console.warn(`âš ï¸ External attendee not found in mapping data: ${attendeeEmail} - SKIPPING to avoid 500 error`);
      // Skip unmapped attendees to prevent 500 errors
    }
  }

  return mappedAttendees;
}



// File to store processed entry IDs
const PROCESSED_ENTRIES_FILE = path.join(__dirname, 'processed_entries.json');

// Helper function to load processed entry IDs from file
async function loadProcessedEntries() {
  try {
    const data = await fs.readFile(PROCESSED_ENTRIES_FILE, 'utf8');
    return new Set(JSON.parse(data));
  } catch (error) {
    // File doesn't exist or is invalid, return empty set
    console.log('No existing processed entries file found, starting fresh');
    return new Set();
  }
}

// Helper function to save processed entry IDs to file
async function saveProcessedEntries(processedEntries) {
  try {
    await fs.writeFile(PROCESSED_ENTRIES_FILE, JSON.stringify([...processedEntries]), 'utf8');
  } catch (error) {
    console.error('Error saving processed entries:', error.message);
  }
}

// Helper function to add entry ID to processed list
async function addToProcessedEntries(entryId) {
  try {
    const processedEntries = await loadProcessedEntries();
    processedEntries.add(entryId);
    await saveProcessedEntries(processedEntries);
  } catch (error) {
    console.error('Error adding entry to processed list:', error.message);
  }
}

async function processTimelineEntry(
  entry,
  userCache,
  companyCache,
  activityCache,
  milestoneCache,
  targetInstanceUrl,
  targetInstanceToken,
  sourceInstanceUrl,
  sourceInstanceToken,
  sourceCompanyId,
  targetCompanyId,
  trackingData,
  isSelectiveRetry = false
) {
  console.log("yuva");
  const entryStartTime = new Date();

  try {
    // Check if this entry has already been processed (skip this check in selective retry mode)
    if (!isSelectiveRetry) {
      const processedEntries = await loadProcessedEntries();
      if (processedEntries.has(entry.id)) {
        console.log(`â­ï¸ Skipping already processed entry: ${entry.id}`);
        return { 
          success: true, 
          skipped: true, 
          entryId: entry.id, 
          reason: 'Entry already processed' 
        };
      }
    } else {
      console.log(`ðŸ”„ Selective retry mode: Processing entry ${entry.id} regardless of previous status`);
    }

    // Claude added - Initialize critical variables at the very start to prevent undefined errors
    let userInfo;
    let userId = "1P01E316G9DAPFOLE6SOOUG71XRMN5F3PLER"; // Claude added - always start with fallback
    let userCookie = targetInstanceToken; // Claude added - always start with default token
    console.log('Processing entry:', entry.id || 'unknown');

    // Extract activity details for tracking
    const activityDetails = {
      activityType: entry?.meta?.activityTypeId || null,
      companyName: entry.contexts?.[0]?.lbl || null,
      authorEmail: entry.author?.email || null,
      subject: entry.note?.subject || null
    };

    // Handle user cache - Claude modified this section
    const authorEmail = entry.author?.email;
    if (authorEmail) {
      if (userCache[authorEmail]) {
        // userCache now stores complete user info object or fallback info
        const cachedUserInfo = userCache[authorEmail];
        if (typeof cachedUserInfo === 'object' && cachedUserInfo.GSID) {
          userInfo = cachedUserInfo;
          userId = cachedUserInfo.GSID;
        } else {
          userId = cachedUserInfo; // fallback ID string
          userInfo = { GSID: cachedUserInfo, mappedToSystemAdmin: cachedUserInfo === "1P01E316G9DAPFOLE6SOOUG71XRMN5F3PLER" };
        }
      } else {
        userInfo = await getUserIdByEmail(authorEmail, targetInstanceUrl, targetInstanceToken);
        // Claude fixed - handle both object and string returns from getUserIdByEmail
        if (userInfo && typeof userInfo === 'object' && userInfo.GSID) {
          userId = userInfo.GSID; // Claude added - extract userId from userInfo object
          userCache[authorEmail] = userInfo; // Cache the complete userInfo object
        } else if (typeof userInfo === 'string') {
          userId = userInfo; // Claude added - userInfo is already the fallback ID string
          userCache[authorEmail] = userInfo; // Cache the fallback ID
        } else {
          console.warn(`âš ï¸ No user found for email: ${authorEmail}`);
          userId = "1P01E316G9DAPFOLE6SOOUG71XRMN5F3PLER"; // Claude added - fallback user ID
          userCache[authorEmail] = userId; // Cache the fallback ID
        }
      }

      // Claude added - Get user-specific cookie via playwright
      try {
        userCookie = await getUserCookieWithCache(userInfo.Email, targetInstanceUrl);
        console.log(`ðŸª Got cookie for user: ${userInfo.Email}`);
      } catch (cookieError) {
        console.warn(`âš ï¸ Failed to get cookie for ${authorEmail}, using default token:`, cookieError.message);
        userCookie = targetInstanceToken; // Claude added - fallback to default token
      }
    } else {
      // Claude added - handle case when no author email
      userId = "1P01E316G9DAPFOLE6SOOUG71XRMN5F3PLER"; // Claude added - fallback user ID
      userCookie = targetInstanceToken; // Claude added - use default token when no author
    }

    // Handle company cache
    let companyId;
    const companyLabel = entry.contexts?.[0]?.lbl;
    if (companyLabel) {
      if (companyCache[companyLabel]) {
        companyId = companyCache[companyLabel];
      } else {
        companyId = await getCompanyIdByName(companyLabel, targetInstanceUrl, targetInstanceToken);
        companyCache[companyLabel] = companyId;
      }
    }

    if (!companyId) {
      MigrationTracker.trackFailure(trackingData, entry.id, 'No company ID found', activityDetails);
      return { success: false, reason: 'No company ID found', entryId: entry.id };
    }

    // Handle activity type and subcategory
    let activityTypeId = null;
    let subCategoryId = null;
    const sourceActivityTypeId = entry?.meta?.activityTypeId;
    let activityMapping = { activityTypeId: null, subActivityTypeId: null };

    if (sourceActivityTypeId) {
      const cacheKey = `advanced_${sourceActivityTypeId}`;

      if (activityCache[cacheKey]) {
        activityMapping = activityCache[cacheKey];
      } else {
        const sourceActivityTypes = await getAllActivityTypes(sourceInstanceUrl, sourceInstanceToken, 'source', entry.contexts?.[0]?.id);
        const sourceActivity = sourceActivityTypes.find(type => type.id === sourceActivityTypeId);

        if (sourceActivity) {
          console.log(`ðŸ” Source activity: ${sourceActivity.name} (ID: ${sourceActivityTypeId})`);

          activityMapping = await getAdvancedActivityMapping(
            sourceActivity.name,
            targetInstanceUrl,
            targetInstanceToken,
            sourceInstanceUrl,
            sourceInstanceToken,
            targetCompanyId
          );
          console.log(activityMapping, "activityMapping")
          activityCache[cacheKey] = activityMapping;
        } else {
          console.error(`âŒ Source activity type not found: ${sourceActivityTypeId}`);
        }
      }
    }

    // Claude added - Handle CANCELLATION_REQUEST activity type - use fixed target number
    let cancellationTypeWithNumber = null;
    if (activityMapping?.activityTypeName && normalizeString(activityMapping.activityTypeName) === normalizeString('Cancellation Request')) {
      // Always use the same fixed number for target system
      cancellationTypeWithNumber = 'CANCELLATION_REQUEST_1750245851650';
      console.log(`ðŸ”§ CANCELLATION_REQUEST - using fixed target type: ${cancellationTypeWithNumber}`);
    }

    // Handle milestone type mapping (only for Milestone activities)
    let milestoneMapping = null;
    const oldMilestoneTypeId = entry?.note?.customFields?.milestoneType;

    if (oldMilestoneTypeId && normalizeString(activityMapping?.activityTypeName) === 'milestone') {
      console.log(`ðŸ MILESTONE DETECTED: Processing milestone type mapping...`);

      const milestoneCacheKey = `milestone_${oldMilestoneTypeId}`;

      if (milestoneCache[milestoneCacheKey]) {
        milestoneMapping = milestoneCache[milestoneCacheKey];
      } else {
        milestoneMapping = await getMilestoneTypeMapping(
          oldMilestoneTypeId,
          sourceInstanceUrl,
          sourceInstanceToken,
          targetInstanceUrl,
          targetInstanceToken,
          entry.contexts?.[0]?.id,
          targetCompanyId
        );

        milestoneCache[milestoneCacheKey] = milestoneMapping;

        if (milestoneMapping) {
          console.log(`âœ… Successfully mapped milestone: ${milestoneMapping.mappingRule}`);
        }
      }
    }

    // Claude added - Store Gong info for later processing (after target activity ID is available)
    let isGongActivity = entry?.meta?.source === "GONG_IO" && entry?.meta?.eid;
    let gongEid = entry?.meta?.eid;

    // Claude added - Detect Salesforce activities based on meta.source field and handle lastModifiedByUser mapping
    let isSalesforceActivity = entry?.meta?.source === "SALESFORCE_ACTIVITY";
    let salesforceUserInfo = null;
    
    if (isSalesforceActivity && entry?.lastModifiedByUser?.name) {
      console.log(`ðŸ”µ Processing Salesforce activity for entry ${entry.id}, lastModifiedBy: ${entry.lastModifiedByUser.name}`);
      try {
        salesforceUserInfo = await getUserIdByName(entry.lastModifiedByUser.name, targetInstanceUrl, targetInstanceToken);
        if (salesforceUserInfo) {
          console.log(`âœ… Mapped Salesforce user: ${entry.lastModifiedByUser.name} -> ${salesforceUserInfo.GSID}`);
        } else {
          console.warn(`âš ï¸ Could not map Salesforce user: ${entry.lastModifiedByUser.name}`);
        }
      } catch (error) {
        console.error(`âŒ Error mapping Salesforce user for entry ${entry.id}:`, error.message);
        MigrationTracker.trackError(trackingData, error, `Salesforce user mapping for entry ${entry.id}`);
      }
    }

    // Claude added - Process attachments AFTER userId is defined
    let processedAttachments = [];
    if (entry.attachments && entry.attachments.length > 0) {
      try {
        processedAttachments = await processAttachments(
          entry.attachments,
          companyId,
          companyLabel,
          userId, // Claude note - userId is now properly defined above
          entry.author?.name,
          entry.author?.email,
          targetInstanceUrl,
          targetInstanceToken
        );
      } catch (error) {
        console.error(`Error processing attachments for entry ${entry.id}:`, error.message);
        MigrationTracker.trackError(trackingData, error, `Attachment processing for entry ${entry.id}`);
      }
    }

    // Claude modified - Look up individual user IDs for each internal attendee
    const ModifiedInternalId = [];
    if (entry.note?.customFields?.internalAttendees) {
      for (const att of entry.note.customFields.internalAttendees) {
        try {
          let attendeeUserId;
    
          // Use cache if available
          if (userCache[att.email]) {
            const cachedAttendeeInfo = userCache[att.email];
            if (typeof cachedAttendeeInfo === 'object' && cachedAttendeeInfo.GSID) {
              attendeeUserId = cachedAttendeeInfo.GSID;
            } else {
              attendeeUserId = cachedAttendeeInfo;
            }
          } else {
            // Look up user info by email
            const attendeeUserInfo = await getUserIdByEmail(att.email, targetInstanceUrl, targetInstanceToken);
    
            // Only proceed if the user was actually found (and not the fallback)
            if (attendeeUserInfo?.Email?.toLowerCase() === att.email.toLowerCase()) {
              attendeeUserId = attendeeUserInfo.GSID;
              userCache[att.email] = attendeeUserInfo; // Cache
            } else {
              console.warn(`âš ï¸ Skipping internal attendee with unknown email: ${att.email}`);
              continue; // Skip this attendee
            }
          }
    
          // Push valid internal attendee
          ModifiedInternalId.push({
            ...att,
            id: attendeeUserId,
            userId: attendeeUserId,
            userName: att.name,
            name: att.name,
            email: att.email,
            userType: "USER",
            activeUser: true,
            eid: att.eid || "",
            epp: att.epp || null,
            esys: att.esys || ""
          });
    
          console.log(`âœ… Mapped internal attendee: ${att.email} -> ${attendeeUserId}`);
        } catch (error) {
          console.error(`âŒ Error mapping internal attendee ${att.email}:`, error.message);
          // Optionally skip or handle errors differently
        }
      }
    }
    
    
    

    // Claude added - Map external attendees from source to target
    const mappedExternalAttendees = await mapExternalAttendees(
      entry.note?.customFields?.externalAttendees,
      targetCompanyId
    );

    // Build custom fields
    const customFields = {
  
      internalAttendees: ModifiedInternalId,
      externalAttendees: mappedExternalAttendees // Claude modified - use mapped external attendees
    };

    if (activityMapping?.subActivityTypeId) {
      customFields.Ant__Activity_Subtype__c = activityMapping.subActivityTypeId;
      console.log(`âœ… Added sub-activity type: ${activityMapping.subActivityTypeId}`);
    }

    // Add milestone type if properly mapped
    if (milestoneMapping?.newMilestoneTypeId) {
      customFields.milestoneType = milestoneMapping.newMilestoneTypeId;
      console.log(`âœ… Added milestone type: ${milestoneMapping.newMilestoneTypeId} (${milestoneMapping.mappingRule})`);
    }
    customFields.Ant__ExternalId__c = entry.id;
    let externalSourceDetails = {}
    //Need to  verify
    if (entry?.meta?.externalSourceDetails?.externalSystems.length > 0) {
      externalSourceDetails = { externalSystems: entry?.meta?.externalSourceDetails?.externalSystems }
    }
    console.log(activityMapping?.activityTypeId,"activityMapping?.activityTypeId")
    const draftPayload = {
      // Claude added - Use Salesforce user mapping if available, otherwise use regular userId
      lastModifiedByUser: {
        gsId: isSalesforceActivity && salesforceUserInfo ? salesforceUserInfo.GSID : userId,
        name: isSalesforceActivity && salesforceUserInfo ? salesforceUserInfo.Name : entry.author?.name,
        eid: entry.lastModifiedByUser?.eid || "",  // Claude modified - preserve eid for CHROME_PLUGIN and other sources
        esys: entry.lastModifiedByUser?.esys || "",
        pp: entry.lastModifiedByUser?.pp || ""
      },
      note: {
        customFields,
        // Claude added - Use preserved or generated CANCELLATION_REQUEST type for filtering
        type: cancellationTypeWithNumber || activityMapping.activityTypeName.toUpperCase(),
        subject: entry.note?.subject?.slice(0, 140),
        activityDate: entry.note?.activityDate,
        content: isGongActivity ? "&lt;p&gt;&lt;/p&gt;": entry.note?.content,
        plainText: isGongActivity ? "_" : (entry.note?.plainText || ""),
        trackers: null
      },
      mentions: [],
      relatedRecords: null,
      meta: {
        externalSourceDetails: externalSourceDetails,
        activityTypeId: activityMapping?.activityTypeId,
        ctaId: null,
        // Claude added - Preserve source from original entry: GONG_IO, SALESFORCE_ACTIVITY, or GLOBAL_TIMELINE
        source: entry?.meta?.source || "GLOBAL_TIMELINE",
        hasTask: false,
        emailSent: false,
        systemType: "GAINSIGHT",
        notesTemplateId: null,
        // Claude added - Include Gong eid for later audio processing
        ...(isGongActivity && { eid: gongEid }),
        // Claude added - Preserve activity sentiment from source
        ...(entry?.meta?.activitySentiment && { activitySentiment: entry.meta.activitySentiment }),
        ...(entry?.meta?.genaiSentimentRationale && { genaiSentimentRationale: entry.meta.genaiSentimentRationale }),
        ...(entry?.meta?.genaiSentimentVerdictReasons && { genaiSentimentVerdictReasons: entry.meta.genaiSentimentVerdictReasons })
      },
      author: {
        id: userId, // Claude modified - use dynamic userId instead of hardcoded value
        obj: "User",
        name: entry.author?.name,
        email: entry.author?.email,
        eid: entry.author?.eid || "",  // Claude modified - preserve eid for CHROME_PLUGIN and other sources
        eobj: "User",
        epp: entry.author?.epp || null,
        esys: entry.author?.esys || "SALESFORCE",
        sys: "GAINSIGHT",
        pp: entry.author?.pp || ""
      },
      syncedToSFDC: false,
      tasks: [],
      attachments: processedAttachments,
      contexts: [
        {
          id: companyId,
          base: true,
          obj: "Company",
          lbl: companyLabel || '',
          eid: entry.contexts?.[0]?.eid,
          eobj: "Account",
          eurl: null,
          esys: "SALESFORCE",
          dsp: true
        }
      ]
    };

    // Add activityTypeId at root level - required by target API
    if (activityMapping?.activityTypeId) {
      draftPayload.activityTypeId = activityMapping.activityTypeId;
      console.log(`âœ… Added activityTypeId to root: ${activityMapping.activityTypeId}`);
    }

    console.log(JSON.stringify(draftPayload), "draftPayload")

    const draftId = await createDraft(draftPayload, targetInstanceUrl, userCookie); // Claude modified - use user-specific cookie
    if (!draftId) {
      // Check if this is a CHROME_PLUGIN specific failure
      const isChromePluxinFailure = entry?.meta?.source === 'CHROME_PLUGIN';
      
      let failureReason;
      if (isChromePluxinFailure) {
        failureReason = `âŒ CHROME_PLUGIN draft failed - ${entry.id}`;
        console.log(`âŒ CHROME_PLUGIN draft creation failed for entry ${entry.id} (${authorEmail})`);
      } else if (userInfo && userInfo.mappedToSystemAdmin) {
        failureReason = `Draft creation failed - Source user inactive/not found in target system (${userInfo.originalEmail})`;
      } else {
        failureReason = 'Draft creation failed';
      }
      
      MigrationTracker.trackFailure(trackingData, entry.id, failureReason, activityDetails);
      return { success: false, reason: failureReason, entryId: entry.id };
    }

    const timelinePayload = { ...draftPayload, id: draftId };

    const postConfig = {
      method: 'post',
      url: `${targetInstanceUrl}/v1/ant/v2/activity`,
      headers: {
        'Content-Type': 'application/json',
        'Cookie': userCookie // Claude modified - use user-specific cookie instead of targetInstanceToken
      },
      data: JSON.stringify(timelinePayload),
      maxBodyLength: Infinity,
      timeout: 60000 // 60 seconds timeout for large payloads
    };console.log(JSON.stringify(timelinePayload), "postConfig")

    const final = await axios(postConfig);
    console.log(final.data, "timelinePostResult");

    // Track successful migration
    const targetActivityId = final.data?.data?.id || draftId;
    
    // Claude added - Process Gong audio after successful timeline creation
    if (isGongActivity && gongEid && targetActivityId) {
      console.log(`ðŸŽµ Processing Gong audio for entry ${entry.id}, eid: ${gongEid}, sourceActivityId: ${entry.id}, targetActivityId: ${targetActivityId}`);
      try {
        const gongAudioData = await fetchGongAudio(gongEid, sourceInstanceUrl, sourceInstanceToken, entry.id);
        if (gongAudioData) {
          console.log(`âœ… Successfully fetched Gong audio data for entry ${entry.id}`);
          console.log(gongAudioData, "gongAudioData");  
          
          
          // Update the timeline entry with Gong audio and video data
          // Claude added - Preserve all existing meta data including sentiment
          const updatePayload = {
            ...timelinePayload,
            meta: {
              ...timelinePayload.meta,
              audioUrl: gongAudioData.audioUrl || null,
              videoUrl: gongAudioData.videoUrl || null,
              callUrl: gongAudioData.callUrl,
              callId: gongAudioData.callId,
              // Claude added - Ensure sentiment data is preserved during update
              ...(entry?.meta?.activitySentiment && { activitySentiment: entry.meta.activitySentiment }),
              ...(entry?.meta?.genaiSentimentRationale && { genaiSentimentRationale: entry.meta.genaiSentimentRationale }),
              ...(entry?.meta?.genaiSentimentVerdictReasons && { genaiSentimentVerdictReasons: entry.meta.genaiSentimentVerdictReasons })
            }
          };
          await addToProcessedEntries(entry.id);
          // Update the timeline entry with audio data
          const updateConfig = {
            method: 'put',
            url: `${targetInstanceUrl}/v1/ant/v2/activity/${targetActivityId}`,
            headers: {
              'Content-Type': 'application/json',
              'Cookie': userCookie
            },
            data: JSON.stringify(updatePayload),
            maxBodyLength: Infinity
          };
          
          await axios(updateConfig);
          console.log(`âœ… Updated timeline entry ${targetActivityId} with Gong audio data`);
          
        }
      } catch (error) {
        console.error(`âŒ Failed to process Gong audio for entry ${entry.id}:`, error.message);
        MigrationTracker.trackError(trackingData, error, `Gong audio processing for entry ${entry.id}`);
        // Don't fail the entire migration for Gong audio issues
      }
    }

    // Add entry ID to processed entries file after successful processing
    await addToProcessedEntries(entry.id);
    console.log(`âœ… Added entry ${entry.id} to processed entries file`);

    // Add saveduser information for detailed results
    // Determine the actual user email that was used to save the activity
    let savedUserEmail = entry.author?.email; // Default to original author
    
    if (userInfo && userInfo.mappedToSystemAdmin) {
      // User was mapped to system admin
      savedUserEmail = 'no-reply@gainsightapp.com';
    } else if (userInfo && userInfo.Email && userInfo.Email !== entry.author?.email) {
      // User was mapped to a different user in target system
      savedUserEmail = userInfo.Email;
    }
    
    const updatedActivityDetails = {
      ...activityDetails,
      saveduser: savedUserEmail
    };

    MigrationTracker.trackSuccess(trackingData, entry.id, targetActivityId, updatedActivityDetails);

    return { success: true, entryId: entry.id, targetId: targetActivityId };

  } catch (error) {
    console.error('Error processing timeline entry:', error.message);
    MigrationTracker.trackError(trackingData, error, `Processing entry ${entry.id}`);

    // Extract activity details for failure tracking
    const activityDetails = {
      activityType: entry?.meta?.activityTypeId || null,
      companyName: entry.contexts?.[0]?.lbl || null,
      authorEmail: entry.author?.email || null,
      subject: entry.note?.subject || null
    };

    MigrationTracker.trackFailure(trackingData, entry.id, error.message, activityDetails);
    return { success: false, reason: error.message, entryId: entry.id };
  }
}

// Optional: Helper function to clear processed entries (for testing or reset)
async function clearProcessedEntries() {
  try {
    await fs.unlink(PROCESSED_ENTRIES_FILE);
    console.log('Processed entries file cleared');
  } catch (error) {
    console.log('No processed entries file to clear');
  }
}

// Optional: Helper function to get count of processed entries
async function getProcessedEntriesCount() {
  try {
    const processedEntries = await loadProcessedEntries();
    return processedEntries.size;
  } catch (error) {
    return 0;
  }
}



// Enhanced batch processing function with tracking
async function processBatch(
  batch,
  userCache,
  companyCache,
  activityCache,
  milestoneCache,
  targetInstanceUrl,
  targetInstanceToken,
  sourceInstanceUrl,
  sourceInstanceToken,
  sourceCompanyId,
  targetCompanyId,
  trackingData,
  batchIndex,
  isSelectiveRetry = false
) {
  const batchStartTime = new Date();
  const results = [];
  let batchSuccessCount = 0;
  let batchFailureCount = 0;

  console.log(`ðŸš€ Starting batch ${batchIndex + 1} with ${batch.length} activities...`);

  // Process items in batch sequentially to avoid overwhelming the API
  for (const entry of batch) {
    try {
      const result = await processTimelineEntry(
        entry,
        userCache,
        companyCache,
        activityCache,
        milestoneCache,
        targetInstanceUrl,
        targetInstanceToken,
        sourceInstanceUrl,
        sourceInstanceToken,
        sourceCompanyId,
        targetCompanyId,
        trackingData,
        isSelectiveRetry
      );

      results.push(result);

      if (result.success) {
        batchSuccessCount++;
      } else {
        batchFailureCount++;
      }

      // Small delay between each entry to prevent rate limiting
      await new Promise(resolve => setTimeout(resolve, 100));

    } catch (error) {
      console.log(error)
      console.error(`Failed to process entry ${entry.id}:`, error.message);
      MigrationTracker.trackError(trackingData, error, `Batch ${batchIndex + 1} - Entry ${entry.id}`);

      const failureResult = {
        success: false,
        reason: error.message,
        entryId: entry.id
      };
      results.push(failureResult);
      batchFailureCount++;
    }
  }

  const batchEndTime = new Date();

  // Track batch timing
  MigrationTracker.trackBatchTiming(
    trackingData,
    batchIndex,
    batch.length,
    batchStartTime,
    batchEndTime,
    batchSuccessCount,
    batchFailureCount
  );

  console.log(`âœ… Completed batch ${batchIndex + 1}: ${batchSuccessCount} successes, ${batchFailureCount} failures`);

  return results;
}

// Helper function to check if an error is a rate limit error
function isRateLimitError(error) {
  if (!error) return false;
  
  // Check for HTTP 429 status
  if (error.response && error.response.status === 429) {
    return true;
  }
  
  // Check for rate limit keywords in error message
  const rateLimitKeywords = [
    'rate limit',
    'too many requests',
    'quota exceeded',
    'throttle',
    'rate exceeded',
    '429'
  ];
  
  const errorMessage = (error.message || '').toLowerCase();
  return rateLimitKeywords.some(keyword => errorMessage.includes(keyword));
}

// Retry logic with exponential backoff for timeline entry processing
async function processTimelineEntryWithRetry(
  entry,
  userCache,
  companyCache,
  activityCache,
  milestoneCache,
  targetInstanceUrl,
  targetInstanceToken,
  sourceInstanceUrl,
  sourceInstanceToken,
  sourceCompanyId,
  targetCompanyId,
  trackingData,
  isSelectiveRetry = false,
  maxRetries = PARALLEL_PROCESSING_CONFIG.MAX_RETRIES
) {
  const baseDelay = 1000; // 1 second base delay
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      // âš¡ Use optimized version with parallel lookups when enabled
      const result = PARALLEL_PROCESSING_CONFIG.ENABLED
        ? await processTimelineEntryOptimized(
            entry,
            userCache,
            companyCache,
            activityCache,
            milestoneCache,
            targetInstanceUrl,
            targetInstanceToken,
            sourceInstanceUrl,
            sourceInstanceToken,
            sourceCompanyId,
            targetCompanyId,
            trackingData,
            isSelectiveRetry
          )
        : await processTimelineEntry(
            entry,
            userCache,
            companyCache,
            activityCache,
            milestoneCache,
            targetInstanceUrl,
            targetInstanceToken,
            sourceInstanceUrl,
            sourceInstanceToken,
            sourceCompanyId,
            targetCompanyId,
            trackingData,
            isSelectiveRetry
          );
      
      // Success - return result
      return result;
      
    } catch (error) {
      console.log(`ðŸ”„ Attempt ${attempt}/${maxRetries} failed for entry ${entry.id}: ${error.message}`);
      
      // If this is the last attempt, throw the error
      if (attempt === maxRetries) {
        console.error(`âŒ Final attempt failed for entry ${entry.id}:`, error.message);
        throw error;
      }
      
      // Check if it's a rate limit error
      if (isRateLimitError(error)) {
        // Exponential backoff for rate limit errors
        const delay = baseDelay * Math.pow(2, attempt - 1);
        console.log(`â±ï¸ Rate limit detected, waiting ${delay}ms before retry ${attempt + 1}...`);
        await new Promise(resolve => setTimeout(resolve, delay));
        continue;
      }
      
      // For non-rate-limit errors, throw immediately
      console.error(`âŒ Non-retryable error for entry ${entry.id}:`, error.message);
      throw error;
    }
  }
}

// âš¡ Parallel lookup function to optimize API calls within processTimelineEntry
async function getParallelLookups(
  entry,
  userCache,
  companyCache,
  activityCache,
  milestoneCache,
  targetInstanceUrl,
  targetInstanceToken,
  sourceInstanceUrl,
  sourceInstanceToken,
  targetCompanyId
) {
  const lookupPromises = [];
  const lookupResults = {
    userInfo: null,
    companyId: null,
    activityMapping: null,
    milestoneMapping: null
  };

  // Prepare user lookup (if not cached)
  const authorEmail = entry.author?.email;
  if (authorEmail && !userCache[authorEmail]) {
    lookupPromises.push(
      getUserIdByEmail(authorEmail, targetInstanceUrl, targetInstanceToken)
        .then(result => ({ type: 'user', result, email: authorEmail }))
        .catch(error => ({ type: 'user', error, email: authorEmail }))
    );
  }

  // Prepare company lookup (if not cached)
  const companyLabel = entry.contexts?.[0]?.lbl;
  if (companyLabel && !companyCache[companyLabel]) {
    lookupPromises.push(
      getCompanyIdByName(companyLabel, targetInstanceUrl, targetInstanceToken)
        .then(result => ({ type: 'company', result, label: companyLabel }))
        .catch(error => ({ type: 'company', error, label: companyLabel }))
    );
  }

  // Prepare activity type lookup (if not cached)
  const sourceActivityTypeId = entry?.meta?.activityTypeId;
  if (sourceActivityTypeId) {
    const cacheKey = `advanced_${sourceActivityTypeId}`;
    if (!activityCache[cacheKey]) {
      // These two lookups can be done in parallel for activity mapping
      const activityLookupPromises = [
        getAllActivityTypes(sourceInstanceUrl, sourceInstanceToken, 'source', entry.contexts?.[0]?.id)
          .then(types => ({ subType: 'sourceTypes', result: types }))
          .catch(error => ({ subType: 'sourceTypes', error }))
      ];

      // First get source activity types, then we'll do the advanced mapping
      lookupPromises.push(
        Promise.all(activityLookupPromises)
          .then(async ([sourceTypesResult]) => {
            if (sourceTypesResult.error) {
              return { type: 'activity', error: sourceTypesResult.error, cacheKey };
            }
            
            const sourceActivity = sourceTypesResult.result.find(type => type.id === sourceActivityTypeId);
            if (!sourceActivity) {
              return { type: 'activity', error: new Error(`Source activity type not found: ${sourceActivityTypeId}`), cacheKey };
            }

            try {
              const activityMapping = await getAdvancedActivityMapping(
                sourceActivity.name,
                targetInstanceUrl,
                targetInstanceToken,
                sourceInstanceUrl,
                sourceInstanceToken,
                targetCompanyId
              );
              return { type: 'activity', result: activityMapping, cacheKey, sourceName: sourceActivity.name };
            } catch (error) {
              return { type: 'activity', error, cacheKey };
            }
          })
          .catch(error => ({ type: 'activity', error, cacheKey }))
      );
    }
  }

  // Execute all lookups in parallel
  if (lookupPromises.length > 0) {
    console.log(`âš¡ Executing ${lookupPromises.length} parallel lookups for entry ${entry.id}...`);
    const results = await Promise.allSettled(lookupPromises);
    
    // Process results
    results.forEach((promiseResult, index) => {
      if (promiseResult.status === 'fulfilled') {
        const lookup = promiseResult.value;
        
        switch (lookup.type) {
          case 'user':
            if (!lookup.error) {
              lookupResults.userInfo = lookup.result;
              userCache[lookup.email] = lookup.result;
              console.log(`âœ… Parallel user lookup completed for: ${lookup.email}`);
            } else {
              console.error(`âŒ Parallel user lookup failed for ${lookup.email}:`, lookup.error.message);
            }
            break;
            
          case 'company':
            if (!lookup.error) {
              lookupResults.companyId = lookup.result;
              companyCache[lookup.label] = lookup.result;
              console.log(`âœ… Parallel company lookup completed for: ${lookup.label}`);
            } else {
              console.error(`âŒ Parallel company lookup failed for ${lookup.label}:`, lookup.error.message);
            }
            break;
            
          case 'activity':
            if (!lookup.error) {
              lookupResults.activityMapping = lookup.result;
              activityCache[lookup.cacheKey] = lookup.result;
              console.log(`âœ… Parallel activity lookup completed for: ${lookup.sourceName}`);
            } else {
              console.error(`âŒ Parallel activity lookup failed:`, lookup.error.message);
            }
            break;
        }
      } else {
        console.error('âŒ Parallel lookup promise rejected:', promiseResult.reason);
      }
    });
  }

  return lookupResults;
}

// âš¡ Optimized version of processTimelineEntry with parallel lookups  
async function processTimelineEntryOptimized(
  entry,
  userCache,
  companyCache,
  activityCache,
  milestoneCache,
  targetInstanceUrl,
  targetInstanceToken,
  sourceInstanceUrl,
  sourceInstanceToken,
  sourceCompanyId,
  targetCompanyId,
  trackingData,
  isSelectiveRetry = false
) {
  // If parallel processing is disabled, use the original function
  if (!PARALLEL_PROCESSING_CONFIG.ENABLED) {
    return await processTimelineEntry(
      entry,
      userCache,
      companyCache,
      activityCache,
      milestoneCache,
      targetInstanceUrl,
      targetInstanceToken,
      sourceInstanceUrl,
      sourceInstanceToken,
      sourceCompanyId,
      targetCompanyId,
      trackingData,
      isSelectiveRetry
    );
  }

  // âš¡ Start with parallel lookups for optimization
  console.log(`âš¡ Starting optimized processing for entry ${entry.id} with parallel lookups...`);
  
  const parallelLookups = await getParallelLookups(
    entry,
    userCache,
    companyCache,
    activityCache,
    milestoneCache,
    targetInstanceUrl,
    targetInstanceToken,
    sourceInstanceUrl,
    sourceInstanceToken,
    targetCompanyId
  );

  // Now process with the original function, which will use cached results
  const result = await processTimelineEntry(
    entry,
    userCache,
    companyCache,
    activityCache,
    milestoneCache,
    targetInstanceUrl,
    targetInstanceToken,
    sourceInstanceUrl,
    sourceInstanceToken,
    sourceCompanyId,
    targetCompanyId,
    trackingData,
    isSelectiveRetry
  );

  console.log(`âœ… Optimized processing completed for entry ${entry.id}: ${result.success ? 'SUCCESS' : 'FAILED'}`);
  return result;
}

// Parallel processing version of processBatch with controlled concurrency
async function processBatchParallel(
  batch,
  userCache,
  companyCache,
  activityCache,
  milestoneCache,
  targetInstanceUrl,
  targetInstanceToken,
  sourceInstanceUrl,
  sourceInstanceToken,
  sourceCompanyId,
  targetCompanyId,
  trackingData,
  batchIndex,
  isSelectiveRetry = false,
  concurrentActivities = 10  // Configurable concurrency
) {
  const batchStartTime = new Date();
  let batchSuccessCount = 0;
  let batchFailureCount = 0;

  console.log(`ðŸš€ Starting PARALLEL batch ${batchIndex + 1} with ${batch.length} activities (concurrency: ${concurrentActivities})...`);

  // Process activities in parallel chunks to control concurrency
  const results = [];
  
  for (let i = 0; i < batch.length; i += concurrentActivities) {
    const chunk = batch.slice(i, i + concurrentActivities);
    const chunkStartTime = new Date();
    
    console.log(`âš¡ Processing chunk ${Math.floor(i / concurrentActivities) + 1} with ${chunk.length} activities in parallel...`);
    
    // Process chunk activities in parallel using Promise.allSettled
    const chunkPromises = chunk.map(entry => 
      processTimelineEntryWithRetry(
        entry,
        userCache,
        companyCache,
        activityCache,
        milestoneCache,
        targetInstanceUrl,
        targetInstanceToken,
        sourceInstanceUrl,
        sourceInstanceToken,
        sourceCompanyId,
        targetCompanyId,
        trackingData,
        isSelectiveRetry
      ).catch(error => {
        // Convert caught errors to failed results
        console.error(`Failed to process entry ${entry.id}:`, error.message);
        MigrationTracker.trackError(trackingData, error, `Parallel Batch ${batchIndex + 1} - Entry ${entry.id}`);
        
        return {
          success: false,
          reason: error.message,
          entryId: entry.id
        };
      })
    );
    
    // Wait for all activities in this chunk to complete
    const chunkResults = await Promise.allSettled(chunkPromises);
    
    // Process results and count successes/failures
    chunkResults.forEach((promiseResult, index) => {
      const result = promiseResult.status === 'fulfilled' ? promiseResult.value : {
        success: false,
        reason: promiseResult.reason?.message || 'Unknown error',
        entryId: chunk[index].id
      };
      
      results.push(result);
      
      if (result.success) {
        batchSuccessCount++;
      } else {
        batchFailureCount++;
      }
    });
    
    const chunkEndTime = new Date();
    const chunkDuration = chunkEndTime - chunkStartTime;
    
    console.log(`âœ… Chunk ${Math.floor(i / concurrentActivities) + 1} completed in ${chunkDuration}ms: ${chunk.filter((_, idx) => results[i + idx]?.success).length} successes, ${chunk.filter((_, idx) => !results[i + idx]?.success).length} failures`);
    
    // Small delay between chunks to prevent overwhelming the API
    if (i + concurrentActivities < batch.length) {
      await new Promise(resolve => setTimeout(resolve, PARALLEL_PROCESSING_CONFIG.CHUNK_DELAY_MS));
    }
  }

  const batchEndTime = new Date();

  // Track batch timing (existing functionality preserved)
  MigrationTracker.trackBatchTiming(
    trackingData,
    batchIndex,
    batch.length,
    batchStartTime,
    batchEndTime,
    batchSuccessCount,
    batchFailureCount
  );

  console.log(`âœ… Completed PARALLEL batch ${batchIndex + 1}: ${batchSuccessCount} successes, ${batchFailureCount} failures`);

  return results;
}

// Fetch complete activity data with contexts field for CHROME_PLUGIN compatibility
async function fetchCompleteActivityData(baseUrl, cookieHeader, activityId) {
  try {
    const url = `${baseUrl}/v1/ant//activity/${activityId}/?meta=true`;
    const headers = {
      'Cookie': cookieHeader,
      'Content-Type': 'application/json'
    };

    console.log(`ðŸ” Fetching complete data for activity: ${activityId}`);
    const response = await axios.get(url, { headers });
    
    if (response.data && response.data.data) {
      console.log(`âœ… Successfully fetched complete data for activity: ${activityId}`);
      return response.data.data;
    } else {
      console.warn(`âš ï¸ No data found for activity: ${activityId}`);
      return null;
    }
  } catch (error) {
    console.error(`âŒ Error fetching complete activity data for ${activityId}:`, error.response?.data || error.message);
    return null;
  }
}

// Claude edited - Modified fetchTimeLinesByEmail to handle single email only
async function fetchTimeLinesByEmail(baseUrl, cookieHeader, email, failedEntryIds = []) {
  console.log(`ðŸ“§ Fetching timelines for email: ${email}...`);
  if (failedEntryIds.length > 0) {
    console.log(`ðŸ” Selective mode: filtering for ${failedEntryIds.length} specific entry IDs`);
  }

  const headers = {
    'Cookie': cookieHeader,
    'Content-Type': 'application/json'
  };

  let page = 0;
  let totalPages = 1;
  const emailData = [];
  const pageSize = 2000;
  const requestBody = {
    searchText: "",
    quickSearch: {},
    filter: {
      advancedFilter: [
        {
          leftOperand: {
            type: "BASE_FIELD",
            fieldName: "Email",
            key: "AuthorId_Email",
            dbName: "gsd41894",
            dataType: "EMAIL",
            label: "Email",
            objectName: "gsuser",
            objectDBName: "user_75aab2e476cc42b8ac9aa3191bbeb0fd",
            objectLabel: "User",
            objectId: "75aab2e4-76cc-42b8-ac9a-a3191bbeb0fd",
            fieldPath: {
              lookupId: "AuthorId__gr",
              lookupName: "AuthorId__gr",
              legacyLookupId: "d8f7413d-8ea4-4494-a2cf-1a4cc6a51b58",
              left: {
                type: "BASE_FIELD",
                fieldName: "Gsid",
                fieldLabel: "GSID",
                label: "GSID",
                dbName: "gsid",
                objectName: "gsuser",
                objectDBName: "user_75aab2e476cc42b8ac9aa3191bbeb0fd",
                hasLookup: false,
                displayOrder: 0
              },
              right: {
                type: "BASE_FIELD",
                fieldName: "AuthorId",
                fieldLabel: "Author Id",
                label: "Author Id",
                dbName: "gs_authorId",
                objectName: "activity_timeline",
                objectDBName: "antv2_activity_view_a87ad17fcaf840018ef9342cb02d7d75",
                hasLookup: false,
                displayOrder: 0
              },
              fieldPath: null
            },
            properties: {
              autoSuggestDetails: {
                object: "gsuser",
                searchOn: ["Email", "Gsid"],
                dataStore: "HAPOSTGRES",
                columnsToList: ["Email"],
                connectionType: "MDA",
                connectionId: "MDA",
                targetFieldName: "Email"
              },
              sourceType: "EMAIL",
              actualDataEditability: {
                accessible: true,
                createable: true,
                updateable: true
              },
              SEARCH_CONTROLLER: "AUTO_SUGGEST",
              pathLabel: "Author Id â†’ Email"
            }
          },
          filterAlias: "A",
          comparisonOperator: "EQ",
          rightOperandType: "VALUE",
          filterValue: {
            value: [email],
            userLiteral: "OTHER_USER"
          }
        }
      ],
      expression: "A",
      dateFilter: null,
      activityTypes: null
    },
    contextFilter: {},
    filterContext: "GLOBAL_TIMELINE"
  };

  try {
    while (page < totalPages) {
      const url = `${baseUrl}/v1/ant/timeline/search/activity?page=${page}&size=${pageSize}`;
      console.log(`â³ Fetching page ${page + 1} for ${email}...`);

      try {
        const response = await axios.post(url, requestBody, { headers });
        const result = response.data.data;
        const pageContent = result.content || [];
        emailData.push(...pageContent);

        totalPages = result.page.totalPages;
        page++;

        console.log(`âœ… Page ${page}/${totalPages} for ${email}: ${pageContent.length} activities`);

      } catch (err) {
        console.error(`âŒ Failed on page ${page + 1} for ${email}:`, err.response?.data || err.message);
        break;
      }
    }

    console.log(`ðŸ“Š Total activities for ${email}: ${emailData.length}`);

    // Filter activities if failedEntryIds is provided
    let filteredActivities = emailData;
    if (failedEntryIds.length > 0) {
      // Debug: Check what ID fields are available in the first few activities
      if (emailData.length > 0) {
        console.log(`ðŸ” Debug: First activity fields:`, Object.keys(emailData[0]));
        console.log(`ðŸ” Debug: First activity ID fields:`, {
          id: emailData[0].id,
          gsid: emailData[0].gsid,
          activityId: emailData[0].activityId,
          sourceActivityId: emailData[0].sourceActivityId
        });
      }
      
      filteredActivities = emailData.filter(activity => 
        failedEntryIds.includes(activity.gsid) || 
        failedEntryIds.includes(activity.id) ||
        failedEntryIds.includes(activity.activityId) ||
        failedEntryIds.includes(activity.sourceActivityId)
      );
      console.log(`ðŸ” Filtered to ${filteredActivities.length} activities matching failed entry IDs`);
      
      if (filteredActivities.length === 0) {
        console.log(`âš ï¸ No matching activities found. Searched for entry IDs: ${failedEntryIds.join(', ')}`);
        console.log(`ðŸ” Sample activity IDs from first 5 activities:`);
        emailData.slice(0, 5).forEach((activity, index) => {
          console.log(`   Activity ${index + 1}: id=${activity.id}, gsid=${activity.gsid}`);
        });
      }
    }

    // Only fetch complete activity data with contexts if we're in selective retry mode
    if (failedEntryIds.length > 0 && filteredActivities.length > 0) {
      console.log(`ðŸ”„ Fetching complete data with contexts for ${filteredActivities.length} activities...`);
      const enrichedActivities = [];
      
      for (const activity of filteredActivities) {
        try {
          // Use the correct ID field for fetching complete data
          const activityId = activity.gsid || activity.id || activity.activityId || activity.sourceActivityId;
          const enrichedActivity = await fetchCompleteActivityData(baseUrl, cookieHeader, activityId);
          if (enrichedActivity) {
            enrichedActivities.push(enrichedActivity);
          } else {
            // If enriched fetch fails, use original activity
            enrichedActivities.push(activity);
          }
        } catch (error) {
          const activityId = activity.gsid || activity.id || activity.activityId || activity.sourceActivityId;
          console.error(`âš ï¸ Failed to enrich activity ${activityId}, using original data:`, error.message);
          enrichedActivities.push(activity);
        }
      }
      
      filteredActivities = enrichedActivities;
      console.log(`âœ… Enriched ${enrichedActivities.length} activities with complete context data`);
    } else if (failedEntryIds.length === 0) {
      console.log(`ðŸ“Š Found ${filteredActivities.length} activities for ${email}`);
      console.log(`â„¹ï¸ Normal mode: Activities will be processed through standard pipeline`);
    }

    return {
      email: email,
      totalActivities: filteredActivities.length,
      activities: filteredActivities
    };

  } catch (error) {
    console.error(`âŒ Error processing ${email}:`, error.message);
    return {
      email: email,
      totalActivities: 0,
      activities: [],
      error: error.message
    };
  }
}

// Claude edited - Enhanced email-by-email processing function with comprehensive reporting
exports.migrateTimelinesPerEmail = async (req, res) => {
  // Initialize tracking
  const trackingData = MigrationTracker.initializeTracking();
  try {
    const { sourceInstanceUrl, sourceInstanceToken, targetInstanceUrl, targetInstanceToken, maxActivities = 7, failedEntryIds = [] } = req.body;

    if (!sourceInstanceUrl || !sourceInstanceToken || !targetInstanceUrl || !targetInstanceToken) {
      MigrationTracker.trackError(trackingData, new Error('Missing instance information'), 'Initialization');
      return res.status(400).json({ message: "Missing source or target instance information" });
    }

    console.log(`ðŸŽ¯ Starting per-email timeline migration with ID: ${trackingData.migrationId}`);
    console.log(`ðŸ“Š Max activities per email: ${maxActivities}`);

    // Check if selective retry mode is enabled
    if (failedEntryIds.length > 0) {
      console.log(`ðŸ”„ Selective retry mode enabled for ${failedEntryIds.length} failed entry IDs`);
      console.log(`ðŸ“‹ Failed entry IDs: ${failedEntryIds.join(', ')}`);
    }

    // Read emails from JSON file
    console.log('ðŸ“§ Reading emails from configuration file...');
    const emails = await readEmailsFromFile(); // Claude edited - use the existing function
    console.log(`ðŸ“ Found ${emails.length} emails to process: ${emails.join(', ')}`);

    // Claude edited - Initialize caches at the top before processing
    const userCache = new Map();
    const companyCache = new Map();
    const activityCache = new Map();
    const milestoneCache = new Map();
    const BATCH_SIZE = 20;

    // Claude edited - Pre-load all reference data BEFORE processing emails
    console.log('ðŸ”„ Pre-loading reference data...');
    let sourceCompanyId, targetCompanyId;

    try {
      const [sourceCompanies, targetCompanies] = await Promise.all([
        getAllCompanies(sourceInstanceUrl, sourceInstanceToken),
        getAllCompanies(targetInstanceUrl, targetInstanceToken)
      ]);

      sourceCompanyId = sourceCompanies?.[0]?.GSID;
      targetCompanyId = targetCompanies?.[0]?.GSID;

      if (!sourceCompanyId || !targetCompanyId) {
        const error = new Error("Could not get company IDs for API calls");
        MigrationTracker.trackError(trackingData, error, 'Reference data loading');

        const finalTracking = MigrationTracker.finalizeTracking(trackingData);
        await MigrationTracker.saveTrackingData(finalTracking);

        return res.status(500).json({
          message: "Could not get company IDs for API calls",
          migrationId: trackingData.migrationId
        });
      }

      console.log(`ðŸ¢ Using source company ID: ${sourceCompanyId}`);
      console.log(`ðŸ¢ Using target company ID: ${targetCompanyId}`);
    } catch (error) {
      console.error('âŒ Error pre-loading reference data:', error.message);
      MigrationTracker.trackError(trackingData, error, 'Reference data loading');

      const finalTracking = MigrationTracker.finalizeTracking(trackingData);
      await MigrationTracker.saveTrackingData(finalTracking);

      return res.status(500).json({
        message: "Failed to load reference data",
        migrationId: trackingData.migrationId,
        error: error.message
      });
    }

    // Claude edited - Continue with email processing (reference data already loaded above)

    // Claude edited - Initialize email-specific tracking
    const emailResults = {};
    let totalActivitiesProcessed = 0;
    let totalSuccessfulMigrations = 0;
    let totalFailedMigrations = 0;

    // Process each email individually
    for (const [emailIndex, email] of emails.entries()) {
      console.log(`\nðŸ“§ Processing email ${emailIndex + 1}/${emails.length}: ${email}`);
      
      // Claude edited - Initialize email-specific tracking data
      const emailTrackingData = MigrationTracker.initializeTracking();
      emailTrackingData.migrationId = `${trackingData.migrationId}_email_${emailIndex + 1}`;
      
      try {
        // Claude edited - Fetch timeline data for this specific email only
        const timelineData = await fetchTimeLinesByEmail(sourceInstanceUrl, sourceInstanceToken, email, failedEntryIds);
        const activities = timelineData.activities;
        
        console.log(`ðŸ“Š Found ${activities.length} activities for ${email}`);
        
        if (activities.length === 0) {
          console.log(`âš ï¸ No activities found for ${email}, skipping...`);
          emailResults[email] = {
            status: 'skipped',
            reason: 'No activities found',
            totalActivities: 0,
            successfulMigrations: 0,
            failedMigrations: 0,
            migrationId: emailTrackingData.migrationId
          };
          continue;
        }

        // Claude edited - Process activities in batches for this email
        const batches = [];
        for (let i = 0; i < activities.length; i += BATCH_SIZE) {
          batches.push(activities.slice(i, i + BATCH_SIZE));
        }

        console.log(`âš™ï¸ Processing ${batches.length} batches for ${email} (${BATCH_SIZE} items per batch)`);
        
        let emailSuccessCount = 0;
        let emailFailureCount = 0;

        // Process each batch for this email
        for (const [batchIndex, batch] of batches.entries()) {
          console.log(`ðŸ”„ Processing batch ${batchIndex + 1}/${batches.length} for ${email}...`);
          
          try {
            // âš¡ Use parallel or sequential processing based on configuration
            const batchResults = PARALLEL_PROCESSING_CONFIG.ENABLED
              ? await processBatchParallel(
                  batch,
                  userCache,
                  companyCache,
                  activityCache,
                  milestoneCache,
                  targetInstanceUrl,
                  targetInstanceToken,
                  sourceInstanceUrl,
                  sourceInstanceToken,
                  sourceCompanyId,
                  targetCompanyId,
                  emailTrackingData,
                  batchIndex,
                  failedEntryIds.length > 0, // Pass selective retry flag
                  PARALLEL_PROCESSING_CONFIG.CONCURRENT_ACTIVITIES
                )
              : await processBatch(
                  batch,
                  userCache,
                  companyCache,
                  activityCache,
                  milestoneCache,
                  targetInstanceUrl,
                  targetInstanceToken,
                  sourceInstanceUrl,
                  sourceInstanceToken,
                  sourceCompanyId,
                  targetCompanyId,
                  emailTrackingData,
                  batchIndex,
                  failedEntryIds.length > 0 // Pass selective retry flag
                );

            // Count successes and failures for this batch
            batchResults.forEach(result => {
              if (result.success) {
                emailSuccessCount++;
              } else {
                emailFailureCount++;
              }
            });

            // Small delay between batches for API rate limiting
            if (batchIndex < batches.length - 1) {
              await new Promise(resolve => setTimeout(resolve, 100));
            }

          } catch (error) {
            console.error(`âŒ Error processing batch ${batchIndex + 1} for ${email}:`, error.message);
            MigrationTracker.trackError(emailTrackingData, error, `Batch ${batchIndex + 1} for ${email}`);
            
            // Mark all items in this batch as failed
            batch.forEach((entry) => {
              MigrationTracker.trackFailure(emailTrackingData, entry.id, `Batch processing error: ${error.message}`, {
                activityType: entry?.meta?.activityTypeId || null,
                companyName: entry.contexts?.[0]?.lbl || null,
                authorEmail: entry.author?.email || null,
                subject: entry.note?.subject || null
              });
              emailFailureCount++;
            });
          }
        }

        // Claude edited - Finalize tracking for this email
        const finalEmailTracking = MigrationTracker.finalizeTracking(emailTrackingData);
        
        // Save email-specific tracking data
        const savedFiles = await MigrationTracker.saveTrackingData(finalEmailTracking);
        
        // Update overall tracking data
        trackingData.statistics.successCount += emailSuccessCount;
        trackingData.statistics.failureCount += emailFailureCount;
        trackingData.successfulMigrations.push(...finalEmailTracking.successfulMigrations);
        trackingData.failedMigrations.push(...finalEmailTracking.failedMigrations);
        
        // Store email results
        emailResults[email] = {
          status: 'completed',
          totalActivities: activities.length,
          successfulMigrations: emailSuccessCount,
          failedMigrations: emailFailureCount,
          successRate: activities.length > 0 ? ((emailSuccessCount / activities.length) * 100).toFixed(2) + '%' : '0%',
          migrationId: finalEmailTracking.migrationId,
          duration: finalEmailTracking.summary.migrationOverview.totalDuration,
          files: savedFiles,
          sampleSuccesses: finalEmailTracking.successfulMigrations.slice(0, 5),
          sampleFailures: finalEmailTracking.failedMigrations.slice(0, 5)
        };

        console.log(`âœ… Completed ${email}: ${emailSuccessCount} successes, ${emailFailureCount} failures`);
        
        // Update totals
        totalActivitiesProcessed += activities.length;
        totalSuccessfulMigrations += emailSuccessCount;
        totalFailedMigrations += emailFailureCount;

      } catch (error) {
        console.error(`âŒ Error processing email ${email}:`, error.message);
        MigrationTracker.trackError(trackingData, error, `Email processing for ${email}`);
        
        emailResults[email] = {
          status: 'failed',
          reason: error.message,
          totalActivities: 0,
          successfulMigrations: 0,
          failedMigrations: 0,
          migrationId: emailTrackingData.migrationId
        };
      }

      // Memory management - clear caches periodically
      if ((emailIndex + 1) % 5 === 0) {
        console.log('ðŸ§¹ Clearing caches to manage memory...');
        userCache.clear();
        companyCache.clear();
        activityCache.clear();
        milestoneCache.clear();
      }
    }

    // Clear caches to free memory
    userCache.clear();
    companyCache.clear();
    activityCache.clear();
    milestoneCache.clear();

    // Finalize overall tracking
    trackingData.totalProcessed = totalActivitiesProcessed;
    const finalTrackingData = MigrationTracker.finalizeTracking(trackingData);
    
    // Save overall tracking data
    const savedFiles = await MigrationTracker.saveTrackingData(finalTrackingData);

    // Print console summary
    MigrationTracker.printConsoleSummary(finalTrackingData);

    console.log('\nðŸŽ‰ Email-by-email migration completed!');
    console.log(`ðŸ“Š Total emails processed: ${emails.length}`);
    console.log(`ðŸ“Š Total activities processed: ${totalActivitiesProcessed}`);
    console.log(`âœ… Total successful migrations: ${totalSuccessfulMigrations}`);
    console.log(`âŒ Total failed migrations: ${totalFailedMigrations}`);

    // Generate comprehensive response
    const response = {
      message: "Email-by-email migration completed successfully",
      migrationId: finalTrackingData.migrationId,
      totalEmailsProcessed: emails.length,
      totalActivitiesProcessed: totalActivitiesProcessed,
      totalSuccessfulMigrations: totalSuccessfulMigrations,
      totalFailedMigrations: totalFailedMigrations,
      overallSuccessRate: totalActivitiesProcessed > 0 ? ((totalSuccessfulMigrations / totalActivitiesProcessed) * 100).toFixed(2) + '%' : '0%',
      duration: finalTrackingData.summary.migrationOverview.totalDuration,
      files: savedFiles,
      emailResults: emailResults,
      summary: {
        completedEmails: Object.values(emailResults).filter(r => r.status === 'completed').length,
        skippedEmails: Object.values(emailResults).filter(r => r.status === 'skipped').length,
        failedEmails: Object.values(emailResults).filter(r => r.status === 'failed').length,
        topPerformingEmails: Object.entries(emailResults)
          .filter(([_, result]) => result.status === 'completed')
          .sort(([_, a], [__, b]) => b.successfulMigrations - a.successfulMigrations)
          .slice(0, 5)
          .map(([email, result]) => ({
            email,
            successfulMigrations: result.successfulMigrations,
            totalActivities: result.totalActivities,
            successRate: result.successRate
          }))
      }
    };

    res.status(200).json(response);

  } catch (error) {
    console.error("âŒ Migration error:", error.message);
    MigrationTracker.trackError(trackingData, error, 'Main migration process');

    // Finalize tracking even in case of error
    const finalTrackingData = MigrationTracker.finalizeTracking(trackingData);
    await MigrationTracker.saveTrackingData(finalTrackingData);
    MigrationTracker.printConsoleSummary(finalTrackingData);

    res.status(500).json({
      message: "Error during email-by-email migration",
      migrationId: finalTrackingData.migrationId,
      error: error.message,
      partialResults: {
        totalProcessed: finalTrackingData.totalProcessed,
        successful: finalTrackingData.statistics.successCount,
        failed: finalTrackingData.statistics.failureCount,
        duration: finalTrackingData.summary?.migrationOverview?.totalDuration || 'Unknown'
      }
    });
  }
};

// Main migration function with enhanced tracking and multi-email support
exports.migrateTimelines = async (req, res) => {
  // Initialize tracking
  const trackingData = MigrationTracker.initializeTracking();
  try {
    const { sourceInstanceUrl, sourceInstanceToken, targetInstanceUrl, targetInstanceToken, maxActivities = 7, failedEntryIds = [] } = req.body;

    if (!sourceInstanceUrl || !sourceInstanceToken || !targetInstanceUrl || !targetInstanceToken) {
      MigrationTracker.trackError(trackingData, new Error('Missing instance information'), 'Initialization');
      return res.status(400).json({ message: "Missing source or target instance information" });
    }

    console.log(`ðŸŽ¯ Starting timeline migration with ID: ${trackingData.migrationId}`);
    console.log(`ðŸ“Š Target: ${maxActivities} activities`);
    
    // âš¡ Log processing mode configuration
    if (PARALLEL_PROCESSING_CONFIG.ENABLED) {
      console.log(`âš¡ Processing mode: PARALLEL (${PARALLEL_PROCESSING_CONFIG.CONCURRENT_ACTIVITIES} concurrent activities per batch)`);
      console.log(`ðŸ”„ Retry logic: ${PARALLEL_PROCESSING_CONFIG.ENABLE_RETRY_LOGIC ? `ENABLED (max ${PARALLEL_PROCESSING_CONFIG.MAX_RETRIES} retries)` : 'DISABLED'}`);
      console.log(`â±ï¸ Chunk delay: ${PARALLEL_PROCESSING_CONFIG.CHUNK_DELAY_MS}ms between chunks`);
    } else {
      console.log(`ðŸ”„ Processing mode: SEQUENTIAL (email-by-email processing)`);
    }

    // Read emails from JSON file
    console.log('ðŸ“§ Reading emails from configuration file...');
    const emails = ["john.ramos@verizonconnect.com"]
    console.log(`ðŸ“ Found ${emails.length} emails to process: ${emails.join(', ')}`);

    // Add failed entry IDs for selective retry (leave empty array for normal processing)
    const failedEntryIdsOverride = ["1I004SG7RDV06L1HFJ5NZJW5MTM5JRO0KQAX"
    ];
    
    // Use override if provided, otherwise use request body
    const finalFailedEntryIds = failedEntryIdsOverride.length > 0 ? failedEntryIdsOverride : failedEntryIds;

    // Check if selective retry mode is enabled
    if (finalFailedEntryIds.length > 0) {
      console.log(`ðŸ”„ Selective retry mode enabled for ${finalFailedEntryIds.length} failed entry IDs`);
      console.log(`ðŸ“‹ Failed entry IDs: ${finalFailedEntryIds.join(', ')}`);
    }

    // Claude edited - Initialize caches at the top before processing
    const userCache = new Map();
    const companyCache = new Map();
    const activityCache = new Map();
    const milestoneCache = new Map();

    // Claude edited - Pre-load all reference data BEFORE processing emails
    console.log('ðŸ”„ Pre-loading reference data...');
    let sourceCompanyId, targetCompanyId;

    
    try {
      const [sourceCompanies, targetCompanies] = await Promise.all([
        getAllCompanies(sourceInstanceUrl, sourceInstanceToken),
        getAllCompanies(targetInstanceUrl, targetInstanceToken)
      ]);

      sourceCompanyId = sourceCompanies?.[0]?.GSID;
      targetCompanyId = targetCompanies?.[0]?.GSID;

      if (!sourceCompanyId || !targetCompanyId) {
        const error = new Error("Could not get company IDs for API calls");
        MigrationTracker.trackError(trackingData, error, 'Reference data loading');

        const finalTracking = MigrationTracker.finalizeTracking(trackingData);
        await MigrationTracker.saveTrackingData(finalTracking);

        return res.status(500).json({
          message: "Could not get company IDs for API calls",
          migrationId: trackingData.migrationId
        });
      }

      console.log(`ðŸ¢ Using source company ID: ${sourceCompanyId}`);
      console.log(`ðŸ¢ Using target company ID: ${targetCompanyId}`);
    } catch (error) {
      console.error('âŒ Error pre-loading reference data:', error.message);
      MigrationTracker.trackError(trackingData, error, 'Reference data loading');

      const finalTracking = MigrationTracker.finalizeTracking(trackingData);
      await MigrationTracker.saveTrackingData(finalTracking);

      return res.status(500).json({
        message: "Failed to load reference data",
        migrationId: trackingData.migrationId,
        error: error.message
      });
    }

    // Claude edited - Pre-process emails to categorize admin vs non-admin users
    console.log('ðŸ” Pre-processing emails to categorize admin vs non-admin users...');
    
    const adminEmails = [];
    const nonAdminEmails = [];
    const userMappingStatus = {};
    
    // Check each email against target users to determine if they need admin mapping
    for (const email of emails) {
      try {
        const userInfo = await getUserIdByEmail(email, targetInstanceUrl, targetInstanceToken);
        
        if (userInfo && userInfo.mappedToSystemAdmin) {
          adminEmails.push(email);
          userMappingStatus[email] = { isAdmin: true, originalEmail: email };
          console.log(`ðŸ‘¤ ${email} -> Will use system admin (no-reply@gainsightapp.com)`);
        } else {
          nonAdminEmails.push(email);
          userMappingStatus[email] = { isAdmin: false, originalEmail: email };
          console.log(`âœ… ${email} -> Found in target system`);
        }
      } catch (error) {
        console.warn(`âš ï¸ Error checking user ${email}, defaulting to admin:`, error.message);
        adminEmails.push(email);
        userMappingStatus[email] = { isAdmin: true, originalEmail: email };
      }
    }
    
    console.log(`ðŸ“Š Categorization complete:`);
    console.log(`   ðŸ”´ Admin users (${adminEmails.length}): ${adminEmails.join(', ')}`);
    console.log(`   ðŸŸ¢ Non-admin users (${nonAdminEmails.length}): ${nonAdminEmails.join(', ')}`);
    
    // Process admin emails first, then non-admin emails
    const sortedEmails = [...adminEmails, ...nonAdminEmails];
    
    console.log('ðŸ“¥ Processing emails (admin users first, then non-admin users)...');
    console.log(sortedEmails);
    
    // ðŸš€ ENTERPRISE QUEUE PROCESSING MODE (for 40M+ records)
    const useEnterpriseQueue = process.env.USE_QUEUE_PROCESSING === 'true' && migrationQueueManager;
    
    if (useEnterpriseQueue && sortedEmails.length > 10) {
      console.log('ðŸš€ [ENTERPRISE MODE] Detected large dataset, using enterprise queue system');
      console.log(`ðŸ“Š Processing ${sortedEmails.length} users with enterprise queue architecture`);
      
      try {
        const queueResult = await migrationQueueManager.enhancedMigration(
          sortedEmails,
          sourceInstanceUrl,
          sourceInstanceToken,
          targetInstanceUrl,
          targetInstanceToken,
          trackingData.migrationId
        );

        console.log('âœ… [ENTERPRISE MODE] Migration queued successfully');
        console.log(`ðŸ“Š Estimated completion time: ${queueResult.estimatedCompletionTime} hours`);
        
        // Update tracking data for queue mode
        trackingData.mode = 'enterprise_queue';
        trackingData.totalUsers = sortedEmails.length;
        trackingData.estimatedCompletionTime = queueResult.estimatedCompletionTime;

        const finalTracking = MigrationTracker.finalizeTracking(trackingData);
        await MigrationTracker.saveTrackingData(finalTracking);

        return res.json({
          message: "Enterprise migration queued successfully",
          migrationId: trackingData.migrationId,
          mode: 'enterprise_queue',
          totalUsers: sortedEmails.length,
          estimatedCompletionTime: queueResult.estimatedCompletionTime,
          queuedUsers: queueResult.queuedUsers,
          status: 'queued'
        });

      } catch (queueError) {
        console.error('âŒ [ENTERPRISE MODE] Queue processing failed:', queueError.message);
        console.log('ðŸ”„ [ENTERPRISE MODE] Falling back to parallel processing');
        // Continue to parallel/sequential processing below
      }
    }
    
    // ðŸš€ ENTERPRISE COOKIE PRE-LOADING (MASSIVE SPEED IMPROVEMENT)
    const useCookiePreloading = enterpriseCookieManager && sortedEmails.length > 1;
    
    if (useCookiePreloading) {
      console.log('ðŸš€ [COOKIE OPTIMIZATION] Pre-loading cookies for all users...');
      console.log('âš¡ This eliminates Playwright calls during processing - MASSIVE speed boost!');
      
      try {
        const cookieResult = await enterpriseCookieManager.preloadCookiesForUsers(
          sortedEmails, 
          targetInstanceUrl
        );
        
        console.log('âœ… [COOKIE OPTIMIZATION] Cookie pre-loading complete!');
        console.log(`ðŸ“Š Loaded cookies for ${cookieResult.successCount}/${cookieResult.totalUsers} users`);
        console.log(`âš¡ Expected speed improvement: 10x+ faster activity processing`);
        
        if (cookieResult.failureCount > 0) {
          console.warn(`âš ï¸ [COOKIE OPTIMIZATION] ${cookieResult.failureCount} users failed cookie loading`);
          console.warn(`   These users will fall back to individual Playwright calls`);
        }
        
      } catch (cookieError) {
        console.error('âŒ [COOKIE OPTIMIZATION] Cookie pre-loading failed:', cookieError.message);
        console.log('ðŸ”„ [COOKIE OPTIMIZATION] Falling back to individual cookie retrieval');
      }
    }
    
    var totalActivitiesProcessed = 0;
    var emailBreakdown = {};

    // ðŸ”„ PARALLEL/SEQUENTIAL PROCESSING MODE (existing functionality)
    console.log('ðŸ”„ Using parallel/sequential processing mode');
    
    // Process each email individually - fetch, process, then move to next
    for (var i = 0; i < sortedEmails.length; i++) {
      const email = sortedEmails[i];
      userCookie=""
      const emailStatus = userMappingStatus[email];
      console.log(`\nðŸ“§ Processing email ${i + 1}/${sortedEmails.length}: ${email} ${emailStatus.isAdmin ? '(Admin)' : '(Regular)'}`);
      
      try {
        // Fetch timeline data for this specific email only
        const timelineData = await fetchTimeLinesByEmail(sourceInstanceUrl, sourceInstanceToken, email, finalFailedEntryIds);
        const activities = timelineData.activities;
        
        console.log(`ðŸ“Š Found ${activities.length} activities for ${email}`);
        
        if (activities.length === 0) {
          console.log(`âš ï¸ No activities found for ${email}, skipping...`);
          emailBreakdown[email] = {
            totalActivities: 0,
            successfulMigrations: 0,
            failedMigrations: 0,
            status: 'skipped'
          };
          continue;
        }

        // Process activities in batches for this email immediately
        const BATCH_SIZE = 20;
        const batches = [];
        for (let j = 0; j < activities.length; j += BATCH_SIZE) {
          batches.push(activities.slice(j, j + BATCH_SIZE));
        }

        console.log(`âš™ï¸ Processing ${batches.length} batches for ${email} (${BATCH_SIZE} items per batch)`);
        
        let emailSuccessCount = 0;
        let emailFailureCount = 0;

        // Process each batch for this email
        for (const [batchIndex, batch] of batches.entries()) {
          console.log(`ðŸ”„ Processing batch ${batchIndex + 1}/${batches.length} for ${email}...`);
          
          try {
            // âš¡ Use parallel or sequential processing based on configuration
            const batchResults = PARALLEL_PROCESSING_CONFIG.ENABLED
              ? await processBatchParallel(
                  batch,
                  userCache,
                  companyCache,
                  activityCache,
                  milestoneCache,
                  targetInstanceUrl,
                  targetInstanceToken,
                  sourceInstanceUrl,
                  sourceInstanceToken,
                  sourceCompanyId,
                  targetCompanyId,
                  trackingData,
                  batchIndex,
                  finalFailedEntryIds.length > 0, // Pass selective retry flag
                  PARALLEL_PROCESSING_CONFIG.CONCURRENT_ACTIVITIES
                )
              : await processBatch(
                  batch,
                  userCache,
                  companyCache,
                  activityCache,
                  milestoneCache,
                  targetInstanceUrl,
                  targetInstanceToken,
                  sourceInstanceUrl,
                  sourceInstanceToken,
                  sourceCompanyId,
                  targetCompanyId,
                  trackingData,
                  batchIndex,
                  finalFailedEntryIds.length > 0 // Pass selective retry flag
                );

            // Count successes and failures for this batch
            batchResults.forEach(result => {
              if (result.success) {
                emailSuccessCount++;
              } else {
                emailFailureCount++;
              }
            });

            // Small delay between batches for API rate limiting
            if (batchIndex < batches.length - 1) {
              await new Promise(resolve => setTimeout(resolve, 100));
            }

          } catch (error) {
            console.error(`âŒ Error processing batch ${batchIndex + 1} for ${email}:`, error.message);
            
            // Mark all items in this batch as failed
            batch.forEach((entry) => {
              MigrationTracker.trackFailure(trackingData, entry.id, `Batch processing error: ${error.message}`, {
                activityType: entry?.meta?.activityTypeId || null,
                companyName: entry.contexts?.[0]?.lbl || null,
                authorEmail: entry.author?.email || null,
                subject: entry.note?.subject || null
              });
              emailFailureCount++;
            });
          }
        }

        // Store email results
        emailBreakdown[email] = {
          totalActivities: activities.length,
          successfulMigrations: emailSuccessCount,
          failedMigrations: emailFailureCount,
          successRate: activities.length > 0 ? ((emailSuccessCount / activities.length) * 100).toFixed(2) + '%' : '0%',
          status: 'completed'
        };

        console.log(`âœ… Completed ${email}: ${emailSuccessCount} successes, ${emailFailureCount} failures`);
        totalActivitiesProcessed += activities.length;

      } catch (error) {
        console.error(`âŒ Error processing email ${email}:`, error.message);
        MigrationTracker.trackError(trackingData, error, `Email processing for ${email}`);
        
        emailBreakdown[email] = {
          totalActivities: 0,
          successfulMigrations: 0,
          failedMigrations: 0,
          status: 'failed',
          error: error.message
        };
      }

      // Memory management - clear caches after each email
      if ((i + 1) % 3 === 0) {
        console.log('ðŸ§¹ Clearing caches to manage memory...');
        userCache.clear();
        companyCache.clear();
        activityCache.clear();
        milestoneCache.clear();
      }
    }

    trackingData.statistics.totalCount = totalActivitiesProcessed;

    console.log(`ðŸ“Š Total timeline entries processed: ${totalActivitiesProcessed}`);
    console.log(`ðŸ“§ Emails processed: ${sortedEmails.length}`);

    if (totalActivitiesProcessed === 0) {
      const finalTracking = MigrationTracker.finalizeTracking(trackingData);
      await MigrationTracker.saveTrackingData(finalTracking);
      
      // Save user-specific logs even if no activities processed
      const userLogs = await MigrationTracker.saveUserSpecificLogs(finalTracking, emailBreakdown);

      // Update successful GSIDs database (will be empty but maintains structure)
      const gsidUpdate = await MigrationTracker.updateSuccessfulGSIDs(finalTracking);

      return res.status(200).json({
        message: "No timeline entries found to migrate",
        migrationId: trackingData.migrationId,
        totalProcessed: 0,
        successful: 0,
        failed: 0,
        userLogs: userLogs,
        gsidTracking: gsidUpdate,
        emailBreakdown: emailBreakdown
      });
    }

    // Claude edited - Reference data already loaded above, clear caches to free memory at the end
    userCache.clear();
    companyCache.clear();
    activityCache.clear();
    milestoneCache.clear();

    // Finalize tracking and generate summary
    const finalTrackingData = MigrationTracker.finalizeTracking(trackingData);

    // Save tracking data to files
    const savedFiles = await MigrationTracker.saveTrackingData(finalTrackingData);

    // Save user-specific migration logs
    const userLogs = await MigrationTracker.saveUserSpecificLogs(finalTrackingData, emailBreakdown);

    // Update successful GSIDs database
    const gsidUpdate = await MigrationTracker.updateSuccessfulGSIDs(finalTrackingData);

    // Print console summary
    MigrationTracker.printConsoleSummary(finalTrackingData);

    console.log('ðŸŽ‰ Migration completed!');

    const response = {
      message: "Migration completed successfully",
      migrationId: finalTrackingData.migrationId,
      totalProcessed: finalTrackingData.totalProcessed,
      successful: finalTrackingData.statistics.successCount,
      failed: finalTrackingData.statistics.failureCount,
      duration: finalTrackingData.summary.migrationOverview.totalDuration,
      successRate: finalTrackingData.summary.statistics.successRate,
      summary: finalTrackingData.summary,
      files: savedFiles,
      userLogs: userLogs,
      gsidTracking: gsidUpdate,
      emailBreakdown: emailBreakdown,
      userMappingStatus: userMappingStatus,
      performance: {
        activitiesPerSecond: finalTrackingData.summary.statistics.activitiesPerSecond,
        averageTimePerBatch: finalTrackingData.summary.statistics.averageTimePerBatch,
        emailsProcessed: sortedEmails.length,
        adminUsersCount: adminEmails.length,
        nonAdminUsersCount: nonAdminEmails.length,
        pageSize: 2000
      },
      nextSteps: {
        totalActivitiesProcessed: finalTrackingData.totalProcessed,
        readyForNextPhase: finalTrackingData.statistics.successCount > (finalTrackingData.totalProcessed * 0.8)
      }
    };

    // Include sample of failed entries if there are any (limit to first 20 for larger batches)
    if (finalTrackingData.failedMigrations.length > 0) {
      response.sampleFailedEntries = finalTrackingData.failedMigrations.slice(0, 20).map(failure => ({
        entryId: failure.entryId, // Added entry ID
        sourceActivityId: failure.sourceActivityId,
        reason: failure.reason,
        errorCode: failure.errorCode,
        companyName: failure.companyName,
        authorEmail: failure.authorEmail
      }));

      if (finalTrackingData.failedMigrations.length > 20) {
        response.note = `Showing first 20 of ${finalTrackingData.failedMigrations.length} failed entries. Check log files for complete details.`;
      }
    }

    // Include sample of successful entries
    if (finalTrackingData.successfulMigrations.length > 0) {
      response.sampleSuccessfulEntries = finalTrackingData.successfulMigrations.slice(0, 10).map(success => ({
        entryId: success.entryId, // Added entry ID
        sourceActivityId: success.sourceActivityId,
        targetActivityId: success.targetActivityId,
        companyName: success.companyName,
        authorEmail: success.authorEmail
      }));
    }

    res.status(200).json(response);

  } catch (error) {
    console.error("âŒ Migration error:", error.message);
    MigrationTracker.trackError(trackingData, error, 'Main migration process');

    // Finalize tracking even in case of error
    const finalTrackingData = MigrationTracker.finalizeTracking(trackingData);
    await MigrationTracker.saveTrackingData(finalTrackingData);
    
    // Save user-specific logs even in case of error
    const userLogs = await MigrationTracker.saveUserSpecificLogs(finalTrackingData, emailBreakdown || {});
    
    // Update successful GSIDs database even in case of error (save any successful ones)
    const gsidUpdate = await MigrationTracker.updateSuccessfulGSIDs(finalTrackingData);
    
    MigrationTracker.printConsoleSummary(finalTrackingData);

    res.status(500).json({
      message: "Error during migration",
      migrationId: finalTrackingData.migrationId,
      error: error.message,
      userLogs: userLogs,
      gsidTracking: gsidUpdate,
      partialResults: {
        totalProcessed: finalTrackingData.totalProcessed,
        successful: finalTrackingData.statistics.successCount,
        failed: finalTrackingData.statistics.failureCount,
        duration: finalTrackingData.summary?.migrationOverview?.totalDuration || 'Unknown'
      },
      nextSteps: {
        canRetryFromBatch: Math.floor(finalTrackingData.totalProcessed),
        recommendRestart: finalTrackingData.statistics.successCount < (finalTrackingData.totalProcessed * 0.5)
      }
    });
  }
};

// ðŸš€ ENTERPRISE COOKIE MANAGER INTEGRATION
let enterpriseCookieManager = null;
try {
  const EnterpriseCookieManager = require('../services/enterpriseCookieManager');
  enterpriseCookieManager = new EnterpriseCookieManager();
  console.log('âœ… Enterprise Cookie Manager loaded for massive speed improvements');
} catch (error) {
  console.log('â„¹ï¸ Enterprise Cookie Manager not available, using standard cookie handling');
}

// Export functions for enterprise cookie manager
module.exports.getUserCookieViaPlaywright = getUserCookieViaPlaywright;
module.exports.enterpriseCookieManager = enterpriseCookieManager;









